{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# handing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# optim, scheduler\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "# pytorch-lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "# pre-trained models\n",
    "import timm\n",
    "\n",
    "# augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# logger\n",
    "import wandb\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    data_dir = '../data/'\n",
    "    \n",
    "    device = device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    img_size = 256\n",
    "    epochs = 20\n",
    "    lr = 1e-3 # [1e-3, 0.00025]\n",
    "    batch_size = 64\n",
    "    val_batch_size = 64\n",
    "    \n",
    "    num_workers = 0\n",
    "    \n",
    "    k = 5\n",
    "    seed = 42\n",
    "\n",
    "    train_dataset = None\n",
    "    valid_dataset = None\n",
    "\n",
    "    version = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_get_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(config.img_size, config.img_size),\n",
    "            ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "def valid_get_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(config.img_size, config.img_size),\n",
    "            ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, config, df, mode, transforms=None):\n",
    "        self.config = config\n",
    "        self.before_img_path = df['before_file_path']\n",
    "        self.after_img_path = df['after_file_path']\n",
    "        \n",
    "        self.labels = df['time_delta']\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = []\n",
    "        \n",
    "        # print(f'########################### {mode} dataset loader')\n",
    "        # for image_path in tqdm(self.image_paths):\n",
    "        #     image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "        #     self.images.append(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        before_img = cv2.imread(self.before_img_path[idx], cv2.COLOR_BGR2RGB)\n",
    "        after_img = cv2.imread(self.after_img_path[idx], cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transforms!=None:\n",
    "            before_img = self.transforms(image=before_img)['image']\n",
    "            after_img = self.transforms(image=after_img)['image']\n",
    "\n",
    "        data = {\n",
    "                    'be_img':torch.tensor(before_img, dtype=torch.float32),\n",
    "                    'af_img':torch.tensor(after_img, dtype=torch.float32),\n",
    "                    'label':torch.tensor(label).long(),\n",
    "                }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(plModel, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        chans = config.valid_dataset[0]['be_img'].shape[0]\n",
    "\n",
    "        self.before_model = timm.create_model(model_name='tf_efficientnet_b0_ns', pretrained=True, in_chans=chans) \n",
    "        self.after_model = timm.create_model(model_name='tf_efficientnet_b0_ns', pretrained=True, in_chans=chans) \n",
    "        # [efficientnet_b1_pruned, efficientnet_lite0, resnet34, tf_efficientnet_b0_ns, densenet121]\n",
    "        num_classes = 1\n",
    "\n",
    "        if hasattr(self.before_model, \"fc\"):\n",
    "            nb_ft = self.before_model.fc.in_features\n",
    "            nb_ft = self.after_model.fc.in_features\n",
    "            self.before_model.fc = nn.Linear(nb_ft, num_classes)\n",
    "            self.after_model.fc = nn.Linear(nb_ft, num_classes)\n",
    "        elif hasattr(self.before_model, \"_fc\"):\n",
    "            nb_ft = self.before_model._fc.in_features\n",
    "            nb_ft = self.after_model._fc.in_features\n",
    "            self.before_model._fc = nn.Linear(nb_ft, num_classes)\n",
    "            self.after_model._fc = nn.Linear(nb_ft, num_classes)\n",
    "        elif hasattr(self.before_model, \"classifier\"):\n",
    "            nb_ft = self.before_model.classifier.in_features\n",
    "            nb_ft = self.after_model.classifier.in_features\n",
    "            self.before_model.classifier = nn.Linear(nb_ft, num_classes)\n",
    "            self.after_model.classifier = nn.Linear(nb_ft, num_classes)\n",
    "        elif hasattr(self.before_model, \"last_linear\"):\n",
    "            nb_ft = self.before_model.last_linear.in_features\n",
    "            nb_ft = self.after_model.last_linear.in_features\n",
    "            self.before_model.last_linear = nn.Linear(nb_ft, num_classes)\n",
    "            self.after_model.last_linear = nn.Linear(nb_ft, num_classes)\n",
    "        \n",
    "        ############################################## Loss \n",
    "        self.criterion = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.before_model(x1)\n",
    "        out2 = self.after_model(x2)\n",
    "        out = out2-out1\n",
    "        return out\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "                            self.config.train_dataset,\n",
    "                            batch_size=self.config.batch_size,\n",
    "                            num_workers=self.config.num_workers,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            pin_memory=True,\n",
    "                        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "                            self.config.valid_dataset,\n",
    "                            batch_size=self.config.val_batch_size,\n",
    "                            num_workers=self.config.num_workers,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            pin_memory=True,\n",
    "                        )\n",
    "        return loader\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        pred = self.forward(train_batch['be_img'], train_batch['af_img'])\n",
    "        loss = self.criterion(pred, train_batch['label'])\n",
    "        \n",
    "        pred = torch.sigmoid(pred)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
    "        return {'loss':loss, 'pred':pred.clone().detach().cpu(), 'label':train_batch['label'].clone().detach().cpu()}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        \n",
    "        preds = torch.cat([x['pred'] for x in outputs])\n",
    "        labels = torch.cat([x['label'] for x in outputs])\n",
    "        mse = torch.sum((labels-preds.squeeze())**2) / len(labels)\n",
    "        \n",
    "        self.log(\"total_train_loss\", avg_loss, logger=True)\n",
    "        self.log(\"total_train_mse\", mse, logger=True)\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        pred = self.forward(val_batch['be_img'], val_batch['af_img'])\n",
    "        loss = self.criterion(pred, val_batch['label'])\n",
    "        \n",
    "        pred = torch.sigmoid(pred)\n",
    "        self.log(\"val_loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, 'pred':pred.clone().detach().cpu(), 'label':val_batch['label'].clone().detach().cpu()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        \n",
    "        preds = torch.cat([x['pred'] for x in outputs])\n",
    "        labels = torch.cat([x['label'] for x in outputs])\n",
    "        mse = torch.sum((labels-preds.squeeze())**2) / len(labels)\n",
    "        \n",
    "        self.log(\"total_val_loss\", avg_loss, logger=True)\n",
    "        self.log(\"total_val_mse\", mse, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config.lr, weight_decay=1e-3)\n",
    "        \n",
    "        # scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=1)\n",
    "        scheduler_cosine = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        # scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=1, total_epoch=5, after_scheduler=scheduler_cosine)\n",
    "        \n",
    "        return [optimizer], [scheduler_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    df_train = pd.read_csv('../data/train.csv')\n",
    "    df_train['before_file_path'] = df_train['before_file_path'].apply(lambda x: x.replace('.png', '_resize256.png'))\n",
    "    df_train['after_file_path'] = df_train['after_file_path'].apply(lambda x: x.replace('.png', '_resize256.png'))\n",
    "    df_train['split'] = df_train['before_file_path'].apply(lambda x: x.split('adjust/')[-1][:5]) + '_' + df_train['time_delta'].astype(str)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config.k, random_state=config.seed, shuffle=True)\n",
    "    n_splits = list(skf.split(df_train, df_train['split']))\n",
    "    \n",
    "    df_train['n_fold'] = -1\n",
    "    for i in range(config.k):\n",
    "        df_train.loc[n_splits[i][1], 'n_fold'] = i\n",
    "    print(df_train['n_fold'].value_counts())\n",
    "    \n",
    "    for fold in range(config.k):\n",
    "        config.start_time = time.strftime('%Y-%m-%d %H:%M', time.localtime(time.time())).replace(' ', '_')\n",
    "        \n",
    "        \n",
    "        logger = WandbLogger(name=f\"{config.start_time}_{config.version}_{config.k}fold_{fold}\", \n",
    "                                     project='dacon-plant', \n",
    "                                     config={key:config.__dict__[key] for key in config.__dict__.keys() if '__' not in key},\n",
    "                                    )\n",
    "    \n",
    "    \n",
    "        tt = df_train.loc[df_train['n_fold']!=fold].reset_index(drop=True).iloc[:1000]\n",
    "        vv = df_train.loc[df_train['n_fold']==fold].reset_index(drop=True)\n",
    "        \n",
    "        train_transforms = train_get_transforms()\n",
    "        valid_transforms = valid_get_transforms()\n",
    "        \n",
    "        config.train_dataset = PlantDataset(config, tt, mode='train', transforms=train_transforms)\n",
    "        config.valid_dataset = PlantDataset(config, vv, mode='valid', transforms=valid_transforms)\n",
    "        \n",
    "        print('train_dataset input shape, label : ', config.train_dataset[0]['be_img'].shape, config.train_dataset[0]['af_img'].shape, config.train_dataset[0]['label'])\n",
    "        print('valid_dataset input shape, label : ', config.valid_dataset[0]['be_img'].shape, config.valid_dataset[0]['af_img'].shape, config.valid_dataset[0]['label'])\n",
    "        \n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch') # ['epoch', 'step']\n",
    "        checkpoints = ModelCheckpoint('model/'+config.version, monitor='total_val_loss', mode='min', filename=f'{config.k}fold_{fold}__' + '{epoch}_{total_val_loss:.4f}')\n",
    "        \n",
    "        model = plModel(config)\n",
    "        trainer = pl.Trainer(\n",
    "                            max_epochs=config.epochs, \n",
    "                            gpus=1, \n",
    "                            log_every_n_steps=50,\n",
    "                            # gradient_clip_val=1000, gradient_clip_algorithm='value', # defalut : [norm, value]\n",
    "                            amp_backend='native', precision=16, # amp_backend default : native\n",
    "                            callbacks=[checkpoints, lr_monitor], \n",
    "                            logger=logger\n",
    "                            ) \n",
    "        \n",
    "        trainer.fit(model)\n",
    "        del model, trainer\n",
    "        wandb.finish()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    2767\n",
      "0    2767\n",
      "1    2767\n",
      "4    2766\n",
      "3    2766\n",
      "Name: n_fold, dtype: int64\n",
      "train_dataset input shape, label :  torch.Size([3, 256, 256]) torch.Size([3, 256, 256]) tensor(1)\n",
      "valid_dataset input shape, label :  torch.Size([3, 256, 256]) torch.Size([3, 256, 256]) tensor(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhho1028\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">2021-12-08_14:50_baseline_5fold_0</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hho1028/dacon-plant\" target=\"_blank\">https://wandb.ai/hho1028/dacon-plant</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hho1028/dacon-plant/runs/263jomnr\" target=\"_blank\">https://wandb.ai/hho1028/dacon-plant/runs/263jomnr</a><br/>\n",
       "                Run data is saved locally in <code>wandb\\run-20211208_145059-263jomnr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type         | Params\n",
      "----------------------------------------------\n",
      "0 | before_model | EfficientNet | 4.0 M \n",
      "1 | after_model  | EfficientNet | 4.0 M \n",
      "2 | criterion    | L1Loss       | 0     \n",
      "----------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n",
      "16.035    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   7%|▋         | 4/58 [00:04<01:06,  1.24s/it, loss=8.12, v_num=omnr, train_loss=8.270, val_loss_step=7.030, val_loss_epoch=8.320]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06be3f89a33791bf2bed5c1b358d2c83091f530d03001d02647f2b5cfc546480"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ray_pl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
