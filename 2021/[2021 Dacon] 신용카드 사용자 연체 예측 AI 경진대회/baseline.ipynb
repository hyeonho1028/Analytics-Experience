{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"baseline.ipynb","provenance":[],"collapsed_sections":["vTH861R6Rfvb"],"toc_visible":true,"mount_file_id":"1viyeSGxbO0eYkoFX769qOYNQH4V5ka3S","authorship_tag":"ABX9TyNP1kgzkRjY66AuRkVCkkJ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ibfhkwliKl0n","executionInfo":{"status":"ok","timestamp":1621858483492,"user_tz":-540,"elapsed":1019,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}}},"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","from sklearn.metrics import log_loss\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm.notebook import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dvsu9JBXKqck","executionInfo":{"status":"ok","timestamp":1621858484671,"user_tz":-540,"elapsed":1182,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"ebbeef43-1096-4253-e7a0-2a9659bf5ca2"},"source":["# 1) credit 열은 해당자가 그 행에 대응되는 신용카드를 발급받은 이후 데이터 수집일까지의 대금 연체 정도로 평가한 값인가요?\n","!unzip /content/drive/MyDrive/old_dacon/dacon_신용/open.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/old_dacon/dacon_신용/open.zip\n","   creating: open/\n","  inflating: open/train.csv          \n","  inflating: open/sample_submission.csv  \n","  inflating: open/test.csv           \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZ97EtiMs706","executionInfo":{"status":"ok","timestamp":1621858857692,"user_tz":-540,"elapsed":17381,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}}},"source":["# df_train = pd.read_csv('open/train.csv')\n","# df_test = pd.read_csv('open/test.csv')\n","# sub = pd.read_csv('open/sample_submission.csv')\n","\n","# # df_train = df_train.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","# # df_test = df_test.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","\n","# df_train.loc[df_train['child_num'] > df_train['family_size'], 'family_size'] = 4\n","# df_train.loc[(df_train['family_type']=='Married') & (df_train['family_size']<2), 'family_size'] = 3\n","# df_train.loc[df_train['child_num'] == df_train['family_size'], 'family_size'] = 2\n","\n","# df_test.loc[df_test['child_num'] > df_test['family_size'], 'family_size'] = 4\n","# df_test.loc[(df_test['family_type']=='Married') & (df_test['family_size']<2), 'family_size'] = 3\n","\n","# df_train['uid_rows'] = ['_'.join(df_train.loc[idx, 'gender':'family_size'].astype(str).tolist()) for idx in tqdm(range(len(df_train)))]\n","# df_test['uid_rows'] = ['_'.join(df_test.loc[idx, 'gender':'family_size'].astype(str).tolist()) for idx in tqdm(range(len(df_test)))]\n","\n","# df_train = pd.concat([df_train, df_train.groupby('uid_rows')[['index']].rank().rename(columns={'index':'rank'})], 1)\n","# df_test['rank'] = 0\n","\n","train = df_train.copy()\n","test = df_test.copy()\n","\n","# 부양가족\n","train['n_dependents2'] = train['family_size'] - train['child_num']\n","test['n_dependents2'] = test['family_size'] - test['child_num']\n","\n","TARGET = 'credit'\n","\n","for col in ['gender', 'car', 'reality', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email']:\n","    temp = train[col].value_counts(True).to_dict()\n","    for df in [train, test]:\n","        df[col] = df[col].map(temp)\n","\n","train['DAYS_BIRTH_DATE'] = train['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","test['DAYS_BIRTH_DATE'] = test['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","\n","# 미고용 상태 NULL\n","train['DAYS_EMPLOYED_DATE'] = train['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","test['DAYS_EMPLOYED_DATE'] = test['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","train['DAYS_EMPLOYED_DATE'] = pd.to_datetime(train['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","test['DAYS_EMPLOYED_DATE'] = pd.to_datetime(test['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","\n","\n","train['begin_month_DATE'] = train['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","test['begin_month_DATE'] = test['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","\n","for df in [train, test]:\n","\n","    # 2019년 기준 현재 나이, 정년까지 남은 연수\n","    df['age'] = 2019 - df['DAYS_BIRTH_DATE'].dt.year\n","    df['age2'] = 60 - df['age']\n","    df['income_total_y'] = df['income_total'] * (df['DAYS_EMPLOYED_DATE'].dt.year + df['DAYS_EMPLOYED_DATE'].dt.month)\n","\n","    df['DAYS_BIRTH_month'] = df['DAYS_BIRTH_DATE'].dt.month\n","    df['DAYS_BIRTH_week'] = df['DAYS_BIRTH_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_BIRTH_day'] = df['DAYS_BIRTH_DATE'].dt.day\n","    df['DAYS_BIRTH_weekday'] = df['DAYS_BIRTH_DATE'].dt.weekday\n","\n","    df['DAYS_EMPLOYED_DATE_month'] = df['DAYS_EMPLOYED_DATE'].dt.month\n","    df['DAYS_EMPLOYED_DATE_week'] = df['DAYS_EMPLOYED_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_EMPLOYED_DATE_day'] = df['DAYS_EMPLOYED_DATE'].dt.day\n","    df['DAYS_EMPLOYED_DATE_weekday'] = df['DAYS_EMPLOYED_DATE'].dt.weekday\n","\n","    df['employment_days'] = (df['DAYS_BIRTH_DATE'] - df['DAYS_EMPLOYED_DATE']).dt.days\n","    df['employment_days'] = df['employment_days']//12\n","\n","    df.loc[df['DAYS_EMPLOYED_DATE']=='2020-01-01', ['DAYS_EMPLOYED_DATE_month', 'DAYS_EMPLOYED_DATE_week', 'DAYS_EMPLOYED_DATE_day', 'DAYS_EMPLOYED_DATE_weekday', 'employment_days']] = -1\n","\n","    df['begin_month_year'] = df['begin_month_DATE'].dt.year\n","    df['begin_month_month'] = df['begin_month_DATE'].dt.month\n","    \n","def create_features(train, test, uid, feature, aggs):\n","    tr, te = train.copy(), test.copy()\n","    \n","    if len(uid)==3:\n","        uid1, uid2, uid3 = uid[0], uid[1], uid[2]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str) + '_' + tr[uid3].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str) + '_' + te[uid3].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{uid3}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    \n","    elif len(uid)==2:\n","        uid1, uid2 = uid[0], uid[1]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    else:\n","        uid1 = uid[0]\n","        tr['uid'] = tr[uid1].astype(str) + '_'\n","        te['uid'] = te[uid1].astype(str) + '_'\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}__{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    tr = tr.drop(columns='uid')\n","    te = te.drop(columns='uid')\n","    return tr, te\n","\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month', ['mean', 'std', 'max'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_year', ['mean', 'std'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_month', ['mean', 'std'])\n","\n","train = train.fillna(-99)\n","test = test.fillna(-99)\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","for data in [train, test]:\n","    # data['EMPLOYED_BIRTH_RATIO'] = data['DAYS_EMPLOYED']/data['DAYS_BIRTH']\n","    # data['INCOME_EMPLOYED_RATIO'] = data['income_total']/data['DAYS_EMPLOYED']\n","    # data['INCOME_BIRTH_RATIO'] = data['income_total']/data['DAYS_BIRTH']\n","    data['personal_id'] = data['gender'].astype(str) + \"_\" + data['DAYS_BIRTH'].astype(str) + \"_\" + data['income_total'].astype(str) + \"_\" + data['income_type'].astype(str)\n","\n","for col in ['personal_id']:\n","    temp = train[col].value_counts(True).to_dict()\n","    for df in [train, test]:\n","        df[col] = df[col].map(temp)\n","\n","cluster_train = df_train.copy()\n","cluster_test = df_test.copy()\n","for col in ['gender', 'car', 'reality', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email']:\n","    cluster_train = pd.concat([cluster_train, pd.get_dummies(cluster_train[col], prefix=col)], 1)\n","    cluster_test = pd.concat([cluster_test, pd.get_dummies(cluster_test[col], prefix=col)], 1)\n","    cluster_train = cluster_train.drop(columns=col)\n","    cluster_test = cluster_test.drop(columns=col)\n","\n","cluster_train.drop(columns='credit', inplace=True)\n","cluster_train.drop(columns=['index', 'rank', 'uid_rows'], inplace=True)\n","cluster_test.drop(columns=['index', 'rank', 'uid_rows'], inplace=True)\n","\n","scaler = MinMaxScaler()\n","scaler.fit(cluster_train.values)\n","\n","cluster_train = scaler.transform(cluster_train.values)\n","cluster_test = scaler.transform(cluster_test.values)\n","\n","km = KMeans(n_clusters=100, random_state=42, max_iter=1000, )\n","# km = AgglomerativeClustering(n_clusters=100, )\n","\n","km.fit(cluster_train)\n","\n","km_train = km.predict(cluster_train)\n","km_test = km.predict(cluster_test)\n","\n","train['cluster'] = km_train\n","test['cluster'] = km_test\n","\n","drop_col = ['index', 'FLAG_MOBIL', 'DAYS_BIRTH_DATE', 'DAYS_EMPLOYED_DATE', 'begin_month_DATE', 'DAYS_EMPLOYED', 'DAYS_BIRTH',\n","            'begin_month_year', 'begin_month_month', 'uid_rows', 'rank', 'income_total_y',]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5N-4kq32bA7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCVcQeBq2bEY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jMQQ5e-2bQo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFIhliXsaRyc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGvQ6eQf2bTX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2rv8Jmy2bXI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vTH861R6Rfvb"},"source":["## rf, xgb"]},{"cell_type":"code","metadata":{"id":"e0LbseMcOZTi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621695233780,"user_tz":-540,"elapsed":163631,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"643bb8a1-65c5-46a7-91c2-6810f655d157"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","FOLDS = 8\n","RANDOM_STATE = 0\n","\n","gk = GroupKFold(n_splits=FOLDS)\n","oof_rf = np.zeros([len(train), 3])\n","pred_rf = np.zeros([len(test), 3])\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","scaler = MinMaxScaler()\n","scaler.fit(train[features])\n","\n","for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","    X_train = scaler.transform(train[features])\n","    X_test = scaler.transform(test[features])\n","    y_train = train[TARGET].values\n","\n","    clf = RandomForestClassifier(n_estimators=1000, max_depth=8,\n","                         random_state=RANDOM_STATE)\n","    clf.fit(X_train[trn_idx], y_train[trn_idx])\n","\n","    oof_rf[val_idx] = clf.predict_proba(X_train[val_idx])\n","    pred_rf += clf.predict_proba(X_test) / FOLDS\n","    print(idx, 'fold complete ---\\n')\n","\n","log_loss(y_train, oof_rf)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 fold complete ---\n","\n","1 fold complete ---\n","\n","2 fold complete ---\n","\n","3 fold complete ---\n","\n","4 fold complete ---\n","\n","5 fold complete ---\n","\n","6 fold complete ---\n","\n","7 fold complete ---\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.7810939217101271"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"uVGBdDHVTdWu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621846342952,"user_tz":-540,"elapsed":753524,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"1a0cc083-67c4-491b-f042-c93f9e1134be"},"source":["folds = 8\n","# skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=True)\n","# gk = GroupKFold(n_splits=folds)\n","oof_xgb = np.zeros([len(train), 3])\n","pred_xgb = np.zeros([len(test), 3])\n","features = [col for col in test.columns if col not in drop_col]\n","\n","for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","    X_train = train[features]\n","    X_test = test[features]\n","    y_train = train[TARGET].values\n","    \n","    clf = XGBClassifier(\n","                         objective='multi:softprob', \n","                         num_class=3,\n","                         metrics='mlogloss',\n","                         n_estimators=10000,\n","                         max_depth=8,\n","                         learning_rate=0.03,\n","                         colsample_bytree=0.5,\n","                         subsample=0.7,\n","                         num_leaves=256,\n","                         reg_alpha=0.01,\n","                         reg_lambda=0.01,\n","                         random_state=0,\n","                        )\n","    \n","    evals = [(X_train.loc[trn_idx], y_train[trn_idx]), (X_train.loc[val_idx], y_train[val_idx])]\n","    clf.fit(X_train.loc[trn_idx], y_train[trn_idx], eval_set=evals, eval_metric='mlogloss', early_stopping_rounds=50, verbose=1000)\n","\n","    oof_xgb[val_idx] = clf.predict_proba(X_train.loc[val_idx])\n","    pred_xgb += clf.predict_proba(X_test) / folds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0]\tvalidation_0-mlogloss:1.08342\tvalidation_1-mlogloss:1.08386\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[431]\tvalidation_0-mlogloss:0.410773\tvalidation_1-mlogloss:0.685429\n","\n","[0]\tvalidation_0-mlogloss:1.08412\tvalidation_1-mlogloss:1.08501\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[496]\tvalidation_0-mlogloss:0.388813\tvalidation_1-mlogloss:0.654976\n","\n","[0]\tvalidation_0-mlogloss:1.0834\tvalidation_1-mlogloss:1.0839\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[474]\tvalidation_0-mlogloss:0.407653\tvalidation_1-mlogloss:0.624729\n","\n","[0]\tvalidation_0-mlogloss:1.08439\tvalidation_1-mlogloss:1.08535\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[565]\tvalidation_0-mlogloss:0.377934\tvalidation_1-mlogloss:0.635819\n","\n","[0]\tvalidation_0-mlogloss:1.08408\tvalidation_1-mlogloss:1.08587\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[515]\tvalidation_0-mlogloss:0.396787\tvalidation_1-mlogloss:0.670234\n","\n","[0]\tvalidation_0-mlogloss:1.0831\tvalidation_1-mlogloss:1.08627\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[466]\tvalidation_0-mlogloss:0.417413\tvalidation_1-mlogloss:0.723444\n","\n","[0]\tvalidation_0-mlogloss:1.08332\tvalidation_1-mlogloss:1.0865\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[404]\tvalidation_0-mlogloss:0.440626\tvalidation_1-mlogloss:0.721863\n","\n","[0]\tvalidation_0-mlogloss:1.08303\tvalidation_1-mlogloss:1.08703\n","Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n","\n","Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n","Stopping. Best iteration:\n","[433]\tvalidation_0-mlogloss:0.427293\tvalidation_1-mlogloss:0.752567\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R61T3X0lUGYT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621846347528,"user_tz":-540,"elapsed":365,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"dc3d3151-b82b-4986-dbdc-3c8ae16b4b95"},"source":["log_loss(y_train, oof*0.5 + oof_xgb*0.5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6669427983066565"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnuKx70sk99k","executionInfo":{"status":"ok","timestamp":1621846656487,"user_tz":-540,"elapsed":418,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"67fe29fe-5d0e-4c88-abb5-f2ac6f4dec14"},"source":["log_loss(y_train, oof*0.5 + oof_xgb*0.4 + oof_nn*.1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6699075716823077"]},"metadata":{"tags":[]},"execution_count":146}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"NhPfFeE3kQFT","executionInfo":{"status":"ok","timestamp":1621846671635,"user_tz":-540,"elapsed":457,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"bb1c8692-11d7-4a47-9cdc-d227d4e3a659"},"source":["sub.loc[:, '0':] = pred*0.5 + pred_xgb*0.4 + pred_nn*0.1\n","sub.to_csv('submission_x_l_n.csv', index=False)\n","sub"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26457</td>\n","      <td>0.065058</td>\n","      <td>0.161051</td>\n","      <td>0.773892</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26458</td>\n","      <td>0.300673</td>\n","      <td>0.274650</td>\n","      <td>0.424677</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>26459</td>\n","      <td>0.036857</td>\n","      <td>0.100426</td>\n","      <td>0.862717</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26460</td>\n","      <td>0.060528</td>\n","      <td>0.080429</td>\n","      <td>0.859043</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>26461</td>\n","      <td>0.081846</td>\n","      <td>0.266709</td>\n","      <td>0.651445</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>36452</td>\n","      <td>0.079717</td>\n","      <td>0.263553</td>\n","      <td>0.656729</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>36453</td>\n","      <td>0.260034</td>\n","      <td>0.283022</td>\n","      <td>0.456943</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>36454</td>\n","      <td>0.044962</td>\n","      <td>0.135788</td>\n","      <td>0.819250</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>36455</td>\n","      <td>0.152063</td>\n","      <td>0.299943</td>\n","      <td>0.547993</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>36456</td>\n","      <td>0.110483</td>\n","      <td>0.478715</td>\n","      <td>0.410802</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 4 columns</p>\n","</div>"],"text/plain":["      index         0         1         2\n","0     26457  0.065058  0.161051  0.773892\n","1     26458  0.300673  0.274650  0.424677\n","2     26459  0.036857  0.100426  0.862717\n","3     26460  0.060528  0.080429  0.859043\n","4     26461  0.081846  0.266709  0.651445\n","...     ...       ...       ...       ...\n","9995  36452  0.079717  0.263553  0.656729\n","9996  36453  0.260034  0.283022  0.456943\n","9997  36454  0.044962  0.135788  0.819250\n","9998  36455  0.152063  0.299943  0.547993\n","9999  36456  0.110483  0.478715  0.410802\n","\n","[10000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":148}]},{"cell_type":"code","metadata":{"id":"3_BExncOp0Pc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621846636630,"user_tz":-540,"elapsed":281445,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"e3dd8f86-7452-4869-dae4-972c040b3383"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.preprocessing import MinMaxScaler\n","\n","FOLDS = 5\n","RANDOM_STATE = 0\n","\n","gk = GroupKFold(n_splits=FOLDS)\n","oof_nn = np.zeros([len(train), 3])\n","pred_nn = np.zeros([len(test), 3])\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","scaler = MinMaxScaler()\n","scaler.fit(train[features])\n","\n","for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","    X_train = scaler.transform(train[features])\n","    X_test = scaler.transform(test[features])\n","    y_train = train[TARGET].values\n","\n","    clf = MLPClassifier(hidden_layer_sizes=(2048, 2), max_iter=2000, warm_start=True, momentum=0.01, early_stopping=True,\n","                         random_state=RANDOM_STATE,\n","                        )\n","    clf.fit(X_train[trn_idx], y_train[trn_idx])\n","\n","    oof_nn[val_idx] = clf.predict_proba(X_train[val_idx])\n","    pred_nn += clf.predict_proba(X_test) / FOLDS\n","    print(idx, 'fold complete ---\\n')\n","\n","log_loss(y_train, oof_nn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 fold complete ---\n","\n","1 fold complete ---\n","\n","2 fold complete ---\n","\n","3 fold complete ---\n","\n","4 fold complete ---\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.9168684420194056"]},"metadata":{"tags":[]},"execution_count":144}]},{"cell_type":"code","metadata":{"id":"2Q8e4g_hp0TA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621695327219,"user_tz":-540,"elapsed":558,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"6fc5f529-20ff-4a45-9fd5-307ec741a6b4"},"source":["oof_nn = oof.copy()\n","pred_nn = pred.copy()\n","\n","log_loss(y_train, oof)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9048024576665183"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"XfZsrJxOiyYH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ptm62c8Siya5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBBDrew-iyeb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zseHuqq_iyib"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9rpIZX5NsPN"},"source":["## lgb"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrVf93eWp2Yg","executionInfo":{"status":"ok","timestamp":1621859070712,"user_tz":-540,"elapsed":76373,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"946350b8-0f7c-4f14-8235-dd9a8b09721e"},"source":["FOLDS = 5\n","RANDOM_STATE = 0\n","\n","skf = StratifiedKFold(n_splits=FOLDS, random_state=RANDOM_STATE, shuffle=True)\n","gk = GroupKFold(n_splits=FOLDS)\n","kf = KFold(n_splits=FOLDS, random_state=RANDOM_STATE, shuffle=True)\n","\n","oof = np.zeros([len(train), 3])\n","pred = np.zeros([len(test), 3])\n","lgb_models = {}\n","\n","params = {'dart':1000, 'gbdt':5000, 'rf':5000, 'goss':5000}\n","learn_type = 'gbdt'\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","# for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","# for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'].astype(str) + '_' + train['DAYS_BIRTH_DATE'].dt.year.astype(str))):\n","for idx, (trn_idx, val_idx) in enumerate(skf.split(train, y=train['credit'])):\n","    X_train = train[features]\n","    X_test = test[features]\n","    y_train = train[TARGET].values\n","\n","    # print(train.loc[val_idx]['rank'].value_counts())\n","\n","    clf = LGBMClassifier(\n","                         boosting_type=learn_type,\n","                         objective='multiclass', #['binary', 'multiclass', 'multiclassova']\n","                         num_calsss=3, \n","                         metrics='multi_logloss', #['log_loss', 'multi_logloss', 'multi_error']\n","                         n_estimators=params[learn_type],\n","                         max_depth=32,\n","                         learning_rate=0.03,\n","                         colsample_bytree=0.5,\n","                         subsample=0.7,\n","                         num_leaves=256,\n","                         reg_alpha=0.01,\n","                         reg_lambda=0.01,\n","                         random_state=RANDOM_STATE,\n","                        )\n","    # evals = [(pd.concat([X_train.loc[trn_idx], X_train2]).reset_index(drop=True),\n","    #           np.concatenate([y_train[trn_idx], y_train2])\n","    #                          ), (X_train.loc[val_idx], y_train[val_idx])]\n","    # clf.fit(pd.concat([X_train.loc[trn_idx], X_train2]).reset_index(drop=True),\n","    #           np.concatenate([y_train[trn_idx], y_train2]), eval_set=evals, early_stopping_rounds=100, verbose=500)\n","    evals = [(X_train.loc[trn_idx],y_train[trn_idx]), (X_train.loc[val_idx], y_train[val_idx])]\n","    clf.fit(X_train.loc[trn_idx], y_train[trn_idx], eval_set=evals, early_stopping_rounds=100, verbose=500)\n","    \n","    if idx==0: feature_importances = clf.feature_importances_ / FOLDS\n","    else: feature_importances += clf.feature_importances_ / FOLDS\n","\n","    oof[val_idx] = clf.predict_proba(X_train.loc[val_idx])\n","    pred += clf.predict_proba(X_test) / FOLDS\n","\n","    lgb_models[idx] = clf\n","    print(idx+1, 'fold complete ################################\\n')\n","\n","    # if idx==4:\n","    #     break\n","\n","log_loss(y_train, oof)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[164]\tvalid_0's multi_logloss: 0.409173\tvalid_1's multi_logloss: 0.697694\n","1 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[173]\tvalid_0's multi_logloss: 0.403528\tvalid_1's multi_logloss: 0.677376\n","2 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[194]\tvalid_0's multi_logloss: 0.384438\tvalid_1's multi_logloss: 0.670525\n","3 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[172]\tvalid_0's multi_logloss: 0.402971\tvalid_1's multi_logloss: 0.690319\n","4 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[176]\tvalid_0's multi_logloss: 0.397377\tvalid_1's multi_logloss: 0.691377\n","5 fold complete ################################\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.6854585631568699"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrLizCy2khb5","executionInfo":{"status":"ok","timestamp":1621691908200,"user_tz":-540,"elapsed":901,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"f310533a-d5d4-4e82-f7b8-23debc86f393"},"source":["# 'category' - reality, family_type\n","# 5fold best : 0.6714943537141546\n","# 10fold best : 0.6687885953231191\n","# add 100kmeans / add 100kmeans gb bm / add 100kmeans gb bmy / add 100kmeans gb bmm : 0.6678719604994495 / 0.6672104903128505 / 0.66793124490285 / 0.6684939580526221\n","log_loss(y_train, oof)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6687236534205659"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Skpf_oXPkhez","executionInfo":{"status":"ok","timestamp":1621692017272,"user_tz":-540,"elapsed":720,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"bffbf754-4b78-433e-8d41-8dac6fe9313b"},"source":["# seed 42\n","oof1 = oof.copy()\n","pred1 = pred.copy()\n","log_loss(y_train, oof1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6695582677962895"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54oCe-XhX0Ul","executionInfo":{"status":"ok","timestamp":1621692152568,"user_tz":-540,"elapsed":104031,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"f4708dbe-4284-49aa-bccf-b566b20179ca"},"source":["# seed 0\n","oof2 = oof.copy()\n","pred2 = pred.copy()\n","log_loss(y_train, oof2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6687236534205659"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"6EMTiboEYbZn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6AaVrgdfZZm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9pg-O9ymJff8","executionInfo":{"status":"ok","timestamp":1621691420053,"user_tz":-540,"elapsed":781,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"9c071750-ea9a-4099-8b8d-a3218b43b678"},"source":["pd.DataFrame({'value':feature_importances, \n","              'feature':test[features].columns}).sort_values('value', ascending=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>value</th>\n","      <th>feature</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>8992.6</td>\n","      <td>DAYS_BIRTH</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>8493.4</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_month_std</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>8081.0</td>\n","      <td>begin_month</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>7959.4</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_mean</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>7790.0</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_std</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>7349.2</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_month_mean</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>7278.4</td>\n","      <td>cluster</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>7048.2</td>\n","      <td>employment_days</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6775.4</td>\n","      <td>income_total</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>6432.4</td>\n","      <td>DAYS_BIRTH_week</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>6279.2</td>\n","      <td>DAYS_BIRTH_day</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>6019.8</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_year_std</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>5418.0</td>\n","      <td>DAYS_EMPLOYED_DATE_week</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>5297.2</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_max</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>4779.6</td>\n","      <td>DAYS_EMPLOYED_DATE_day</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>4577.4</td>\n","      <td>DAYS_BIRTH_DAYS_EMPLOYED_begin_month_year_mean</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>3961.6</td>\n","      <td>DAYS_BIRTH_weekday</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>3676.2</td>\n","      <td>occyp_type</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3611.2</td>\n","      <td>age</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>3603.6</td>\n","      <td>DAYS_BIRTH_month</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>3054.0</td>\n","      <td>DAYS_EMPLOYED_DATE_weekday</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2775.8</td>\n","      <td>DAYS_EMPLOYED_DATE_month</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2121.2</td>\n","      <td>age2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1531.4</td>\n","      <td>family_type</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1330.2</td>\n","      <td>family_size</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1127.4</td>\n","      <td>income_type</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1019.6</td>\n","      <td>child_num</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>999.0</td>\n","      <td>edu_type</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>737.6</td>\n","      <td>reality</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>664.0</td>\n","      <td>car</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>620.6</td>\n","      <td>gender</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>594.2</td>\n","      <td>phone</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>539.6</td>\n","      <td>house_type</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>525.8</td>\n","      <td>work_phone</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>359.2</td>\n","      <td>n_dependents2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>254.6</td>\n","      <td>email</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     value                                          feature\n","9   8992.6                                       DAYS_BIRTH\n","34  8493.4   DAYS_BIRTH_DAYS_EMPLOYED_begin_month_month_std\n","15  8081.0                                      begin_month\n","28  7959.4        DAYS_BIRTH_DAYS_EMPLOYED_begin_month_mean\n","29  7790.0         DAYS_BIRTH_DAYS_EMPLOYED_begin_month_std\n","33  7349.2  DAYS_BIRTH_DAYS_EMPLOYED_begin_month_month_mean\n","35  7278.4                                          cluster\n","27  7048.2                                  employment_days\n","4   6775.4                                     income_total\n","20  6432.4                                  DAYS_BIRTH_week\n","21  6279.2                                   DAYS_BIRTH_day\n","32  6019.8    DAYS_BIRTH_DAYS_EMPLOYED_begin_month_year_std\n","24  5418.0                          DAYS_EMPLOYED_DATE_week\n","30  5297.2         DAYS_BIRTH_DAYS_EMPLOYED_begin_month_max\n","25  4779.6                           DAYS_EMPLOYED_DATE_day\n","31  4577.4   DAYS_BIRTH_DAYS_EMPLOYED_begin_month_year_mean\n","22  3961.6                               DAYS_BIRTH_weekday\n","13  3676.2                                       occyp_type\n","17  3611.2                                              age\n","19  3603.6                                 DAYS_BIRTH_month\n","26  3054.0                       DAYS_EMPLOYED_DATE_weekday\n","23  2775.8                         DAYS_EMPLOYED_DATE_month\n","18  2121.2                                             age2\n","7   1531.4                                      family_type\n","14  1330.2                                      family_size\n","5   1127.4                                      income_type\n","3   1019.6                                        child_num\n","6    999.0                                         edu_type\n","2    737.6                                          reality\n","1    664.0                                              car\n","0    620.6                                           gender\n","11   594.2                                            phone\n","8    539.6                                       house_type\n","10   525.8                                       work_phone\n","16   359.2                                    n_dependents2\n","12   254.6                                            email"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"WgF4xJvvJfja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621776042868,"user_tz":-540,"elapsed":253,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"305caae2-8476-492f-a294-a8a48e4ba7f4"},"source":["(pred/5*8).sum(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., ..., 1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"ABblnqYdJfmL"},"source":["log_loss(df_train[TARGET], (oof1*.5 + oof2*.5)*.3 + oof3*.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmPCOADRW9du"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpanC-NMSaU_","colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"status":"ok","timestamp":1621842724138,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"551cccbc-2a57-43ab-8d5b-1fe92b2bab14"},"source":["sub.loc[:, '0':] = pred*5\n","sub.to_csv('submission_holdout.csv', index=False)\n","sub"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26457</td>\n","      <td>0.076873</td>\n","      <td>0.143967</td>\n","      <td>0.779161</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26458</td>\n","      <td>0.170763</td>\n","      <td>0.334556</td>\n","      <td>0.494680</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>26459</td>\n","      <td>0.022345</td>\n","      <td>0.051508</td>\n","      <td>0.926146</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26460</td>\n","      <td>0.071054</td>\n","      <td>0.058465</td>\n","      <td>0.870481</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>26461</td>\n","      <td>0.051354</td>\n","      <td>0.310933</td>\n","      <td>0.637713</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>36452</td>\n","      <td>0.061794</td>\n","      <td>0.104031</td>\n","      <td>0.834175</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>36453</td>\n","      <td>0.229621</td>\n","      <td>0.344777</td>\n","      <td>0.425602</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>36454</td>\n","      <td>0.029437</td>\n","      <td>0.079820</td>\n","      <td>0.890743</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>36455</td>\n","      <td>0.235205</td>\n","      <td>0.429368</td>\n","      <td>0.335427</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>36456</td>\n","      <td>0.143246</td>\n","      <td>0.424692</td>\n","      <td>0.432061</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 4 columns</p>\n","</div>"],"text/plain":["      index         0         1         2\n","0     26457  0.076873  0.143967  0.779161\n","1     26458  0.170763  0.334556  0.494680\n","2     26459  0.022345  0.051508  0.926146\n","3     26460  0.071054  0.058465  0.870481\n","4     26461  0.051354  0.310933  0.637713\n","...     ...       ...       ...       ...\n","9995  36452  0.061794  0.104031  0.834175\n","9996  36453  0.229621  0.344777  0.425602\n","9997  36454  0.029437  0.079820  0.890743\n","9998  36455  0.235205  0.429368  0.335427\n","9999  36456  0.143246  0.424692  0.432061\n","\n","[10000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"P6uVNLxESaX1","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1621557466027,"user_tz":-540,"elapsed":585,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"d6383de8-6313-4b3a-9b52-2cc301565484"},"source":["sub.loc[:, '0':] = pred\n","sub.to_csv('submission_10fold_vc.csv', index=False)\n","sub"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26457</td>\n","      <td>0.081060</td>\n","      <td>0.171615</td>\n","      <td>0.747325</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26458</td>\n","      <td>0.369734</td>\n","      <td>0.285064</td>\n","      <td>0.345202</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>26459</td>\n","      <td>0.032977</td>\n","      <td>0.087273</td>\n","      <td>0.879750</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26460</td>\n","      <td>0.070734</td>\n","      <td>0.069845</td>\n","      <td>0.859421</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>26461</td>\n","      <td>0.057851</td>\n","      <td>0.240005</td>\n","      <td>0.702144</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>36452</td>\n","      <td>0.104379</td>\n","      <td>0.295846</td>\n","      <td>0.599775</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>36453</td>\n","      <td>0.218298</td>\n","      <td>0.235430</td>\n","      <td>0.546272</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>36454</td>\n","      <td>0.043793</td>\n","      <td>0.115590</td>\n","      <td>0.840617</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>36455</td>\n","      <td>0.156328</td>\n","      <td>0.250512</td>\n","      <td>0.593160</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>36456</td>\n","      <td>0.090284</td>\n","      <td>0.481638</td>\n","      <td>0.428078</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 4 columns</p>\n","</div>"],"text/plain":["      index         0         1         2\n","0     26457  0.081060  0.171615  0.747325\n","1     26458  0.369734  0.285064  0.345202\n","2     26459  0.032977  0.087273  0.879750\n","3     26460  0.070734  0.069845  0.859421\n","4     26461  0.057851  0.240005  0.702144\n","...     ...       ...       ...       ...\n","9995  36452  0.104379  0.295846  0.599775\n","9996  36453  0.218298  0.235430  0.546272\n","9997  36454  0.043793  0.115590  0.840617\n","9998  36455  0.156328  0.250512  0.593160\n","9999  36456  0.090284  0.481638  0.428078\n","\n","[10000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"TNIp2cuYwlZ3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXWhX-xvwldl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh9m_2fV0YOQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqnST3P90YQ1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xN2HwhV0YT5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YLH2l5c0YWL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHPGgrx10YZm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsINivro0YfW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_KOadZQbAsk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vktbhE0IbAv4"},"source":["df_train = pd.read_csv('open/train.csv')\n","df_test = pd.read_csv('open/test.csv')\n","sub = pd.read_csv('open/sample_submission.csv')\n","\n","# df_train = df_train.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","# df_test = df_test.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","\n","df_train.loc[df_train['child_num'] > df_train['family_size'], 'family_size'] = 4\n","df_train.loc[(df_train['family_type']=='Married') & (df_train['family_size']<2), 'family_size'] = 3\n","df_train.loc[df_train['child_num'] == df_train['family_size'], 'family_size'] = 2\n","\n","df_test.loc[df_test['child_num'] > df_test['family_size'], 'family_size'] = 4\n","df_test.loc[(df_test['family_type']=='Married') & (df_test['family_size']<2), 'family_size'] = 3\n","\n","# temp = []\n","# duplicates = []\n","# for idx in tqdm(range(len(df_train))):\n","    # row = '_'.join(df_train.loc[idx, 'gender':'family_size'].astype(str).tolist())\n","#     if row in temp:\n","#         duplicates.append(sum([row==i for i in temp]) + 1)\n","#     else:\n","#         duplicates.append(1)\n","    # temp.append(row)\n","\n","# df_train['rank'] = duplicates\n","# df_train['uid_rows'] = temp\n","\n","# duplicates = []\n","# for idx in tqdm(range(len(df_test))):\n","#     row = '_'.join(df_test.loc[idx, 'gender':'family_size'].astype(str).tolist())\n","#     if row in temp:\n","#         duplicates.append(sum([row==i for i in temp]) + 1)\n","#     else:\n","#         duplicates.append(1)\n","#     temp.append(row)\n","\n","# df_test['rank'] = duplicates\n","# df_test['uid_rows'] = temp[-10000:]\n","df_train['uid_rows'] = ['_'.join(df_train.loc[idx, 'gender':'family_size'].astype(str).tolist()) for idx in tqdm(range(len(df_train)))]\n","df_test['uid_rows'] = ['_'.join(df_test.loc[idx, 'gender':'family_size'].astype(str).tolist()) for idx in tqdm(range(len(df_test)))]\n","\n","df_train = pd.concat([df_train, df_train.groupby('uid_rows')[['index']].rank().rename(columns={'index':'rank'})], 1)\n","df_test['rank'] = 0\n","\n","train = df_train.copy()\n","test = df_test.copy()\n","\n","# 부양가족\n","train['n_dependents2'] = train['family_size'] - train['child_num']\n","test['n_dependents2'] = test['family_size'] - test['child_num']\n","\n","TARGET = 'credit'\n","\n","for col in ['gender', 'car', 'reality', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email']:\n","# for col in ['gender', 'car', 'reality', 'child_num', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email', 'family_size']:\n","    temp = train[col].value_counts(True).to_dict()\n","    for df in [train, test]:\n","        df[col] = df[col].map(temp)\n","\n","# test['uid_rows_vc'] = test['uid_rows'].map(train['uid_rows'].value_counts())\n","# train['uid_rows_vc'] = train['uid_rows'].map(train['uid_rows'].value_counts())\n","\n","train['DAYS_EMPLOYED_binary'] = train['DAYS_EMPLOYED']>0\n","test['DAYS_EMPLOYED_binary'] = test['DAYS_EMPLOYED']>0\n","\n","train['DAYS_BIRTH_DATE'] = train['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","test['DAYS_BIRTH_DATE'] = test['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","\n","# 미고용 상태 NULL\n","train['DAYS_EMPLOYED_DATE'] = train['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","test['DAYS_EMPLOYED_DATE'] = test['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","train['DAYS_EMPLOYED_DATE'] = pd.to_datetime(train['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","test['DAYS_EMPLOYED_DATE'] = pd.to_datetime(test['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","\n","\n","train['begin_month_DATE'] = train['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","test['begin_month_DATE'] = test['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","\n","for df in [train, test]:\n","\n","    # 2019년 기준 현재 나이, 정년까지 남은 연수\n","    df['age'] = 2019 - df['DAYS_BIRTH_DATE'].dt.year\n","    df['age2'] = 60 - df['age']\n","    df['income_total_y'] = df['income_total'] * (df['DAYS_EMPLOYED_DATE'].dt.year + df['DAYS_EMPLOYED_DATE'].dt.month)\n","\n","    df['DAYS_BIRTH_month'] = df['DAYS_BIRTH_DATE'].dt.month\n","    df['DAYS_BIRTH_week'] = df['DAYS_BIRTH_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_BIRTH_day'] = df['DAYS_BIRTH_DATE'].dt.day\n","    df['DAYS_BIRTH_weekday'] = df['DAYS_BIRTH_DATE'].dt.weekday\n","\n","    df['DAYS_EMPLOYED_DATE_month'] = df['DAYS_EMPLOYED_DATE'].dt.month\n","    df['DAYS_EMPLOYED_DATE_week'] = df['DAYS_EMPLOYED_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_EMPLOYED_DATE_day'] = df['DAYS_EMPLOYED_DATE'].dt.day\n","    df['DAYS_EMPLOYED_DATE_weekday'] = df['DAYS_EMPLOYED_DATE'].dt.weekday\n","\n","    df['employment_days'] = (df['DAYS_BIRTH_DATE'] - df['DAYS_EMPLOYED_DATE']).dt.days\n","    df['employment_days'] = df['employment_days']//12\n","\n","    df.loc[df['DAYS_EMPLOYED_DATE']=='2020-01-01', ['DAYS_EMPLOYED_DATE_month', 'DAYS_EMPLOYED_DATE_week', 'DAYS_EMPLOYED_DATE_day', 'DAYS_EMPLOYED_DATE_weekday', 'employment_days']] = -1\n","\n","    df['begin_month_year'] = df['begin_month_DATE'].dt.year\n","    df['begin_month_month'] = df['begin_month_DATE'].dt.month\n","    \n","def create_features(train, test, uid, feature, aggs):\n","    tr, te = train.copy(), test.copy()\n","    \n","    if len(uid)==3:\n","        uid1, uid2, uid3 = uid[0], uid[1], uid[2]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str) + '_' + tr[uid3].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str) + '_' + te[uid3].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{uid3}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    \n","    elif len(uid)==2:\n","        uid1, uid2 = uid[0], uid[1]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    else:\n","        uid1 = uid[0]\n","        tr['uid'] = tr[uid1].astype(str) + '_'\n","        te['uid'] = te[uid1].astype(str) + '_'\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}__{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","\n","    tr = tr.drop(columns='uid')\n","    te = te.drop(columns='uid')\n","    temp = tr[new_col].value_counts(True).to_dict()\n","    for df in [tr, te]:\n","        df[new_col] = df[new_col].map(temp)\n","\n","    return tr, te\n","\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month', ['mean', 'std', 'max'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_year', ['mean', 'std'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_month', ['mean', 'std'])\n","\n","# train, test = create_features(train, test, ['cluster'], 'begin_month', ['mean'])\n","# train, test = create_features(train, test, ['occyp_type'], 'income_total', ['mean'])\n","# train, test = create_features(train, test, ['cluster'], 'income_total', ['mean'])\n","\n","train = train.fillna(-99)\n","test = test.fillna(-99)\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","\n","cluster_train = df_train.copy()\n","cluster_test = df_test.copy()\n","for col in ['gender', 'car', 'reality', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email']:\n","    cluster_train = pd.concat([cluster_train, pd.get_dummies(cluster_train[col], prefix=col)], 1)\n","    cluster_test = pd.concat([cluster_test, pd.get_dummies(cluster_test[col], prefix=col)], 1)\n","    cluster_train = cluster_train.drop(columns=col)\n","    cluster_test = cluster_test.drop(columns=col)\n","\n","cluster_train.drop(columns='credit', inplace=True)\n","cluster_train.drop(columns=['index', 'rank', 'uid_rows'], inplace=True)\n","cluster_test.drop(columns=['index', 'rank', 'uid_rows'], inplace=True)\n","\n","scaler = MinMaxScaler()\n","scaler.fit(cluster_train.values)\n","\n","cluster_train = scaler.transform(cluster_train.values)\n","cluster_test = scaler.transform(cluster_test.values)\n","\n","km = KMeans(n_clusters=100, random_state=42, max_iter=1000, )\n","# km = AgglomerativeClustering(n_clusters=100, )\n","\n","km.fit(cluster_train)\n","\n","km_train = km.predict(cluster_train)\n","km_test = km.predict(cluster_test)\n","\n","train['cluster'] = km_train\n","test['cluster'] = km_test\n","\n","drop_col = ['index', 'FLAG_MOBIL', 'DAYS_BIRTH_DATE', 'DAYS_EMPLOYED_DATE', 'begin_month_DATE', 'DAYS_EMPLOYED_binary', 'DAYS_EMPLOYED', 'DAYS_BIRTH',\n","            'begin_month_year', 'begin_month_month', 'uid_rows', 'rank', 'income_total_y',]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwKlTS3ZbAzW"},"source":["# df_train['rank'] = 0\n","# temp_dict = {}\n","# for i in df_train.index:\n","\n","#     key = '_'.join(df_train.loc[i, :'family_size'].astype(str))\n","#     try:\n","#         temp_dict[key]\n","#         temp_dict[key] += 1\n","#     except:\n","#         temp_dict[key] = 0\n","    \n","#     df_train.loc[i, 'rank'] = temp_dict[key]\n","# df_test['rank']=0\n","\n","# df_train['duplicated'] = df_train.loc[:, :'begin_month'].duplicated().astype(int)\n","# df_test['duplicated'] = df_test.loc[:, :'begin_month'].duplicated().astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XqGYyAL4bA2X"},"source":["test = train[22000:].reset_index(drop=True)\n","train = train[:22000].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hff7UmNjbA2Y"},"source":["test = test.drop(columns='credit')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Bg6sOT3bA2Y","executionInfo":{"status":"ok","timestamp":1621305956855,"user_tz":-540,"elapsed":22655,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"329615e7-7297-4bc8-8516-ea9fb28740f5"},"source":["skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n","\n","oof = np.zeros([len(train), 2])\n","pred = np.zeros([len(test), 2])\n","lgb_models = {}\n","\n","params = {'dart':100, 'gbdt':5000, 'rf':5000, 'goss':5000}\n","learn_type = 'gbdt'\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['credit'])):\n","    X_train = train[features]\n","    X_test = test[features]\n","    y_train = train[TARGET].map({0:0, 1:1, 2:1}).values\n","\n","    clf = LGBMClassifier(\n","                         boosting_type=learn_type,\n","                         objective='binary', #['binary', 'multiclass', 'multiclassova']\n","                         metrics='log_loss', #['log_loss', 'multi_logloss', 'multi_error']\n","                         n_estimators=params[learn_type],\n","                         max_depth=64,\n","                         learning_rate=0.03,\n","                         colsample_bytree=0.5,\n","                         subsample=0.7,\n","                         num_leaves=256,\n","                         reg_alpha=0.01,\n","                         reg_lambda=0.01,\n","                         random_state=0,\n","                        )\n","    evals = [(X_train.loc[trn_idx], y_train[trn_idx]), (X_train.loc[val_idx], y_train[val_idx])]\n","    clf.fit(X_train.loc[trn_idx], y_train[trn_idx], eval_set=evals, eval_metric='logloss', early_stopping_rounds=100, verbose=500)\n","\n","    if idx==0: feature_importances = clf.feature_importances_/5\n","    else: feature_importances += clf.feature_importances_/5\n","\n","    oof[val_idx] = clf.predict_proba(X_train.loc[val_idx], num_iteration=clf.best_iteration_)\n","    pred += clf.predict_proba(X_test, num_iteration=clf.best_iteration_) / 5\n","\n","    lgb_models[idx] = clf\n","    print(idx, 'fold complete ################################\\n')\n","\n","log_loss(y_train, oof)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[82]\tvalid_0's binary_logloss: 0.196579\tvalid_1's binary_logloss: 0.31414\n","0 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[101]\tvalid_0's binary_logloss: 0.179188\tvalid_1's binary_logloss: 0.312159\n","1 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[115]\tvalid_0's binary_logloss: 0.167826\tvalid_1's binary_logloss: 0.309396\n","2 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[72]\tvalid_0's binary_logloss: 0.206537\tvalid_1's binary_logloss: 0.325476\n","3 fold complete ################################\n","\n","Training until validation scores don't improve for 100 rounds.\n","Early stopping, best iteration is:\n","[88]\tvalid_0's binary_logloss: 0.187993\tvalid_1's binary_logloss: 0.323812\n","4 fold complete ################################\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.31699643690860324"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"Nz6NO-8kbA2Z"},"source":["train = pd.concat([train, pd.DataFrame(oof, columns=['p1', 'p2'])], 1)\n","test = pd.concat([test, pd.DataFrame(pred, columns=['p1', 'p2'])], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqnEuBXgri15"},"source":["# train['credit'] = train['credit'].map({0:0, 1:1, 2:1})\n","# train = train[train[TARGET] != -1].reset_index(drop=True)\n","\n","target_len = train[TARGET].nunique()\n","folds = 5\n","\n","skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=True)\n","gk = GroupKFold(n_splits=folds)\n","kf = KFold(n_splits=folds, random_state=0, shuffle=True)\n","\n","oof = np.zeros([len(train), target_len])\n","pred = np.zeros([len(test), target_len])\n","lgb_models = {}\n","\n","# y = train['begin_month'].astype(str) + '_' +train['credit'].astype(str)\n","# for trn_idx, val_idx in skf.split(train, y):\n","\n","params = {'dart':1000, 'gbdt':5000, 'rf':5000, 'goss':5000}\n","learn_type = 'gbdt'\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","# for idx, (trn_idx, val_idx) in enumerate(kf.split(train)):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['credit'].map({0:0, 1:1, 2:1}))):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['rank'])):\n","# for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['prior_credit'])):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['begin_month'].astype(str) + '_' + train['credit'].astype(str))):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['credit'])):\n","for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","    X_train = train[features]\n","    X_test = test[features]\n","    y_train = train[TARGET].values\n","\n","    # print(train.loc[val_idx]['rank'].value_counts())\n","\n","    clf = LGBMClassifier(\n","                         boosting_type=learn_type,\n","                         objective='multiclass', #['binary', 'multiclass', 'multiclassova']\n","                         num_calsss=3, \n","                         metrics='multi_logloss', #['log_loss', 'multi_logloss', 'multi_error']\n","                         n_estimators=params[learn_type],\n","                         max_depth=32,\n","                         learning_rate=0.03,\n","                         colsample_bytree=0.5,\n","                         subsample=0.7,\n","                         num_leaves=256,\n","                         reg_alpha=0.01,\n","                         reg_lambda=0.01,\n","                         random_state=0,\n","                        )\n","    evals = [(X_train.loc[trn_idx], y_train[trn_idx]), (X_train.loc[val_idx], y_train[val_idx])]\n","    clf.fit(X_train.loc[trn_idx], y_train[trn_idx], eval_set=evals, early_stopping_rounds=100, verbose=500)\n","\n","    if idx==0: feature_importances = clf.feature_importances_ / folds\n","    else: feature_importances += clf.feature_importances_ / folds\n","\n","    oof[val_idx] = clf.predict_proba(X_train.loc[val_idx])\n","    pred += clf.predict_proba(X_test) / folds\n","\n","    lgb_models[idx] = clf\n","    print(idx, 'fold complete ################################\\n')\n","\n","log_loss(y_train, oof)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Tt5IHuE0Yl_"},"source":["# train['credit'] = train['credit'].map({0:0, 1:1, 2:1})\n","# train = train[train[TARGET] != -1].reset_index(drop=True)\n","\n","target_len = train[TARGET].nunique()\n","\n","skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n","gk = GroupKFold(n_splits=5)\n","kf = KFold(n_splits=5, random_state=0, shuffle=True)\n","\n","oof = np.zeros([len(train), target_len])\n","pred = np.zeros([len(test), target_len])\n","lgb_models = {}\n","\n","# y = train['begin_month'].astype(str) + '_' +train['credit'].astype(str)\n","# for trn_idx, val_idx in skf.split(train, y):\n","\n","params = {'dart':100, 'gbdt':5000, 'rf':5000, 'goss':5000}\n","learn_type = 'gbdt'\n","\n","features = [col for col in test.columns if col not in drop_col]\n","\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['begin_month'].astype(str) + '_' + train['credit'].astype(str))):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['credit'].map({0:0, 1:1, 2:1}))):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['credit'])):\n","# for idx, (trn_idx, val_idx) in enumerate(skf.split(train, train['rank'])):\n","# for idx, (trn_idx, val_idx) in enumerate(kf.split(train)):\n","for idx, (trn_idx, val_idx) in enumerate(gk.split(train, groups=train['rank'])):\n","    X_train = train[features]\n","    X_test = test[features]\n","    y_train = train[TARGET].values\n","\n","    print(train.loc[val_idx]['rank'].value_counts())\n","\n","    clf = LGBMClassifier(\n","                         boosting_type=learn_type,\n","                         objective='multiclass', #['binary', 'multiclass', 'multiclassova']\n","                         num_calsss=3, \n","                         metrics='multi_logloss', #['log_loss', 'multi_logloss', 'multi_error']\n","                         n_estimators=params[learn_type],\n","                         max_depth=64,\n","                         learning_rate=0.03,\n","                         colsample_bytree=0.5,\n","                         subsample=0.7,\n","                         num_leaves=256,\n","                         reg_alpha=0.01,\n","                         reg_lambda=0.01,\n","                         random_state=0,\n","                        )\n","    evals = [(X_train.loc[trn_idx], y_train[trn_idx]), (X_train.loc[val_idx], y_train[val_idx])]\n","    clf.fit(X_train.loc[trn_idx], y_train[trn_idx], eval_set=evals, early_stopping_rounds=100, verbose=500)\n","\n","    if idx==0: feature_importances = clf.feature_importances_/5\n","    else: feature_importances += clf.feature_importances_/5\n","\n","    oof[val_idx] = clf.predict_proba(X_train.loc[val_idx], num_iteration=clf.best_iteration_)\n","    pred += clf.predict_proba(X_test, num_iteration=clf.best_iteration_) / 5\n","\n","    lgb_models[idx] = clf\n","    print(idx, 'fold complete ################################\\n')\n","\n","log_loss(y_train, oof)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhOsCXyW0YqB"},"source":["oof_0 = oof[:, 0]\n","pred_0 = pred[:, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDmwFqJW0YqB"},"source":["oof_temp = oof.copy()\n","pred_temp = pred.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktn9oHDz0YqB"},"source":["oof_temp[:, 0] = oof_0\n","pred_temp[:, 0] = pred_0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08zAsN0t0YqB","executionInfo":{"status":"ok","timestamp":1620961028026,"user_tz":-540,"elapsed":628,"user":{"displayName":"Hyeonho Lee","photoUrl":"https://lh4.googleusercontent.com/-HVS8AXbvopM/AAAAAAAAAAI/AAAAAAAAIcE/9EO2J2RY5vI/s64/photo.jpg","userId":"01233170983161563057"}},"outputId":"6e7dc040-89b9-438e-eb66-274124cdf212"},"source":["log_loss(df_train[TARGET], oof_temp)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6815677554297949"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"HXV7r2Fe0YqC"},"source":["oof_ = np.zeros([len(train), target_len])\n","\n","for iter, (i,j,k) in enumerate(oof_temp):\n","    if sum([i,j,k]) < 1:\n","        d = (1 - i - j - k)/2\n","        # j += d\n","        # k += d\n","    else:\n","        d = (i + j + k - 1)/3\n","        i -= d\n","        j -= d\n","        k -= d\n","        \n","    oof_[iter] = [i,j,k]\n","log_loss(df_train[TARGET], oof_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW28cGdqwlgO"},"source":["# hyper optimization으로 찾아낸 parameter\n","# lightgbm dart 사용, 보다 lb 0.03 정도 좋음\n","# gbdt가 0.3285라면 dart는 0.3255, goss는 0.3300\n","lgb_param_dart = {'objective': 'multiclass', \n"," 'num_class': 19, \n"," 'boosting_type': 'dart', \n"," 'subsample_freq': 5, \n"," 'num_leaves': 92, \n"," 'min_data_in_leaf': 64, \n"," 'subsample_for_bin': 23000, \n"," 'max_depth': 10, \n"," 'feature_fraction': 0.302, \n"," 'bagging_fraction': 0.904, \n"," 'lambda_l1': 0.099, \n"," 'lambda_l2': 1.497, \n"," 'min_child_weight': 38.011, \n"," 'nthread': 32, \n"," 'metric': 'multi_logloss', \n"," 'learning_rate': 0.021, \n"," 'min_sum_hessian_in_leaf': 3, \n"," 'drop_rate': 0.846244, \n"," 'skip_drop': 0.792465, \n"," 'max_drop': 65,\n"," 'seed': 42,\n"," 'n_estimators': 1000}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Lp9waEOE1nu"},"source":["train['phone'] = ((train['phone'] + train['work_phone'])>1).astype(int)\n","test['phone'] = ((test['phone'] + test['work_phone'])>1).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kS471VaSuIR0"},"source":["train['uid'] = (\n","                train['DAYS_BIRTH'].astype(str) + '_' + \n","                # train['car'].astype(str) + '_' + \n","                # train['reality'].astype(str) + '_' + \n","                # train['child_num'].astype(str) + '_' + \n","                # train['income_type'].astype(str) + '_' + \n","                # train['edu_type'].astype(str) + '_' + \n","                train['DAYS_EMPLOYED'].astype(str))\n","\n","test['uid'] = (\n","                test['DAYS_BIRTH'].astype(str) + '_' + \n","                # test['car'].astype(str) + '_' + \n","                # test['reality'].astype(str) + '_' + \n","                # test['child_num'].astype(str) + '_' + \n","                # test['income_type'].astype(str) + '_' + \n","                # test['edu_type'].astype(str) + '_' + \n","                test['DAYS_EMPLOYED'].astype(str))\n","\n","train['ttt'] = train['uid'].map(train.groupby('uid')['begin_month'].mean()).fillna(-99)\n","test['ttt'] = test['uid'].map(train.groupby('uid')['begin_month'].mean()).fillna(-99)\n","train['tttt'] = train['uid'].map(train.groupby('uid')['begin_month'].std()).fillna(-99)\n","test['tttt'] = test['uid'].map(train.groupby('uid')['begin_month'].std()).fillna(-99)\n","\n","# train['uid'] = (\n","#                 train['DAYS_BIRTH'].astype(str) + '_' + \n","#                 # train['gender'].astype(str) + '_' + \n","#                 # train['reality'].astype(str) + '_' + \n","#                 # train['child_num'].astype(str) + '_' + \n","#                 train['income_type'].astype(str) + '_' + \n","#                 # train['edu_type'].astype(str) + '_' + \n","#                 train['DAYS_EMPLOYED'].astype(str))\n","\n","# test['uid'] = (\n","#                 test['DAYS_BIRTH'].astype(str) + '_' + \n","#                 # test['gender'].astype(str) + '_' + \n","#                 # test['reality'].astype(str) + '_' + \n","#                 # test['child_num'].astype(str) + '_' + \n","#                 test['income_type'].astype(str) + '_' + \n","#                 # test['edu_type'].astype(str) + '_' + \n","#                 test['DAYS_EMPLOYED'].astype(str))\n","\n","# train['attt'] = train['uid'].map(train.groupby('DAYS_BIRTH')['begin_month'].mean())\n","# test['attt'] = test['uid'].map(train.groupby('DAYS_BIRTH')['begin_month'].mean()).fillna(-99)\n","# train['atttt'] = train['uid'].map(train.groupby('DAYS_BIRTH')['begin_month'].std())\n","# test['atttt'] = test['uid'].map(train.groupby('DAYS_BIRTH')['begin_month'].std()).fillna(-99)\n","\n","# train['bttt'] = train['DAYS_EMPLOYED'].map(train.groupby('DAYS_EMPLOYED')['begin_month'].mean())\n","# test['bttt'] = test['DAYS_EMPLOYED'].map(train.groupby('DAYS_EMPLOYED')['begin_month'].mean()).fillna(-99)\n","# train['btttt'] = train['DAYS_EMPLOYED'].map(train.groupby('DAYS_EMPLOYED')['begin_month'].std())\n","# test['btttt'] = test['DAYS_EMPLOYED'].map(train.groupby('DAYS_EMPLOYED')['begin_month'].std()).fillna(-99) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eF5q5JAddnC"},"source":["# from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n","# from sklearn.cluster import KMeans, AgglomerativeClustering\n","\n","# cluster_train = df_train.copy()\n","# cluster_test = df_test.copy()\n","# for col in ['gender', 'car', 'reality', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'family_type', 'work_phone', 'phone', 'email']:\n","#     cluster_train = pd.concat([cluster_train, pd.get_dummies(cluster_train[col], prefix=col)], 1)\n","#     cluster_test = pd.concat([cluster_test, pd.get_dummies(cluster_test[col], prefix=col)], 1)\n","#     cluster_train = cluster_train.drop(columns=col)\n","#     cluster_test = cluster_test.drop(columns=col)\n","# cluster_train.drop(columns='credit', inplace=True)\n","# # d = [col for col in cluster_train.columns if 'DATE' not in col]\n","# # cluster_train = cluster_train[d]\n","# # cluster_test = cluster_test[d]\n","\n","# cluster_train = cluster_train[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']]\n","# cluster_test = cluster_test[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']]\n","\n","\n","# scaler = MinMaxScaler()\n","# scaler.fit(cluster_train.values)\n","\n","# cluster_train = scaler.transform(cluster_train.values)\n","# cluster_test = scaler.transform(cluster_test.values)\n","\n","# km = KMeans(n_clusters=100, random_state=42, max_iter=1000, )\n","# # km = AgglomerativeClustering(n_clusters=100, )\n","\n","# km.fit(cluster_train)\n","\n","# km_train = km.predict(cluster_train)\n","# km_test = km.predict(cluster_test)\n","\n","# train['cluster'] = km_train\n","# test['cluster'] = km_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-i0bs_YSaa3"},"source":["# train = pd.concat([train, pd.get_dummies(train['child_num'], prefix='child_num')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['child_num'], prefix='child_num')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['begin_month'], prefix='begin_month')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['begin_month'], prefix='begin_month')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['edu_type'], prefix='edu_type')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['edu_type'], prefix='edu_type')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['house_type'], prefix='house_type')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['house_type'], prefix='house_type')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['occyp_type'], prefix='occyp_type')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['occyp_type'], prefix='occyp_type')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['income_type'], prefix='income_type')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['income_type'], prefix='income_type')], 1)\n","\n","# train = pd.concat([train, pd.get_dummies(train['family_type'], prefix='family_type')], 1)\n","# test = pd.concat([test, pd.get_dummies(test['family_type'], prefix='family_type')], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9i1e5y5Sadu"},"source":["    # t_features = ['t1', 't2', 't3']\n","    # a = train.loc[trn_idx].groupby('DAYS_BIRTH')[TARGET].value_counts(True).unstack().fillna(0.0)\n","    # a.columns = t_features\n","\n","    # for col in t_features:\n","    #     X_train = np.concatenate([X_train, train['DAYS_BIRTH'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","    #     X_test = np.concatenate([X_test, test['DAYS_BIRTH'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","\n","    # t_features = ['t4', 't5', 't6']\n","    # a = train.loc[trn_idx].groupby('begin_month')[TARGET].value_counts(True).unstack().fillna(0.0)\n","    # a.columns = t_features\n","\n","    # for col in t_features:\n","    #     X_train = np.concatenate([X_train, train['begin_month'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","    #     X_test = np.concatenate([X_test, test['begin_month'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","\n","    # t_features = ['t7', 't8', 't9']\n","    # a = train.loc[trn_idx].groupby('DAYS_BIRTH_day')[TARGET].value_counts(True).unstack().fillna(0.0)\n","    # a.columns = t_features\n","\n","    # for col in t_features:\n","    #     X_train = np.concatenate([X_train, train['DAYS_BIRTH_day'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","    #     X_test = np.concatenate([X_test, test['DAYS_BIRTH_day'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","\n","    # t_features = ['t10', 't11', 't12']\n","    # a = train.loc[trn_idx].groupby('DAYS_BIRTH_month')[TARGET].value_counts(True).unstack().fillna(0.0)\n","    # a.columns = t_features\n","\n","    # for col in t_features:\n","    #     X_train = np.concatenate([X_train, train['DAYS_BIRTH_month'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)\n","    #     X_test = np.concatenate([X_test, test['DAYS_BIRTH_month'].map(a[col].to_dict()).values.reshape(-1, 1)], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHsgkMRUSagW"},"source":["# pp를 해봄\n","\n","# adj_arr = []\n","# for i in range(oof.shape[0]):\n","#     temp = oof[i].copy()\n","#     if (oof[i]<0.1).any():\n","#         min = oof[i][np.argmin(oof[i])]\n","\n","#         temp[np.argmin(temp)] = temp[np.argmin(temp)] - min\n","#         temp[np.argmax(temp)] = temp[np.argmax(temp)] + min\n","\n","#     adj_arr += [temp]\n","# adj_arr = np.array(adj_arr)\n","# log_loss(y_train, adj_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fPSnBwjMmcJ"},"source":["# train['edu_type'] = train['edu_type'].astype('category')\n","# test['edu_type'] = test['edu_type'].astype('category')\n","\n","# train['income_type'] = train['income_type'].astype('category')\n","# test['income_type'] = test['income_type'].astype('category')\n","\n","# train['family_type'] = train['family_type'].astype('category')\n","# test['family_type'] = test['family_type'].astype('category')\n","\n","# train['house_type'] = train['house_type'].astype('category')\n","# test['house_type'] = test['house_type'].astype('category')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-VPilSYRfW-"},"source":["# train['DAYS_EMPLOYED_day'] = (train['DAYS_EMPLOYED']/24).round()\n","# test['DAYS_EMPLOYED_day'] = (test['DAYS_EMPLOYED']/24).round()\n","\n","# train['phone2'] = train[['work_phone', 'phone']].sum(1)\n","# test['phone2'] = test[['work_phone', 'phone']].sum(1)\n","\n","# col = 'DAYS_EMPLOYED'\n","# train['income_mean'] = train[col].map(train.groupby(col)['income_total'].mean().to_dict()).round(2)\n","# test['income_mean'] = test[col].map(train.groupby(col)['income_total'].mean().to_dict()).round(2)\n","\n","# col1 = 'DAYS_BIRTH_month'\n","# col2 = 'DAYS_BIRTH_day'\n","# for df in [train, test]:\n","#     df['ttt'] = df[col1].astype(str) + df[col2].astype(str)\n","# test['ttt'] = test['ttt'].map(train['ttt'].value_counts())\n","# train['ttt'] = train['ttt'].map(train['ttt'].value_counts())\n","\n","\n","\n","# train['bigin_year'] = (train['begin_month']/12).round()\n","# test['bigin_year'] = (test['begin_month']/12).round()\n","\n","# date_dict = {i:j for i,j in zip(np.unique(train['begin_month']), pd.date_range('2014-01-01', periods=60, freq='M'))}\n","# train['begin_month_date'] = train['begin_month'].map(date_dict)\n","# test['begin_month_date'] = test['begin_month'].map(date_dict)\n","\n","# train['begin_month_date_month'] = train['begin_month_date'].dt.month\n","# test['begin_month_date_month'] = test['begin_month_date'].dt.month"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETqUYzQkpIS_"},"source":["/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n","  warnings.warn('Early stopping is not available in dart mode')\n","[100]\tvalid_0's multi_logloss: 0.717462\tvalid_1's multi_logloss: 0.795645\n","[200]\tvalid_0's multi_logloss: 0.630438\tvalid_1's multi_logloss: 0.748241\n","[300]\tvalid_0's multi_logloss: 0.569636\tvalid_1's multi_logloss: 0.72224\n","[400]\tvalid_0's multi_logloss: 0.521711\tvalid_1's multi_logloss: 0.703517\n","[500]\tvalid_0's multi_logloss: 0.487992\tvalid_1's multi_logloss: 0.696224\n","[600]\tvalid_0's multi_logloss: 0.456908\tvalid_1's multi_logloss: 0.689985\n","[700]\tvalid_0's multi_logloss: 0.419924\tvalid_1's multi_logloss: 0.683839\n","[800]\tvalid_0's multi_logloss: 0.394576\tvalid_1's multi_logloss: 0.682825\n","[900]\tvalid_0's multi_logloss: 0.358987\tvalid_1's multi_logloss: 0.682866\n","[1000]\tvalid_0's multi_logloss: 0.332996\tvalid_1's multi_logloss: 0.685887\n","0 fold complete ################################\n","\n","[100]\tvalid_0's multi_logloss: 0.725321\tvalid_1's multi_logloss: 0.790456\n","[200]\tvalid_0's multi_logloss: 0.643862\tvalid_1's multi_logloss: 0.739061\n","[300]\tvalid_0's multi_logloss: 0.586349\tvalid_1's multi_logloss: 0.707981\n","[400]\tvalid_0's multi_logloss: 0.53976\tvalid_1's multi_logloss: 0.686057\n","[500]\tvalid_0's multi_logloss: 0.507105\tvalid_1's multi_logloss: 0.676175\n","[600]\tvalid_0's multi_logloss: 0.476674\tvalid_1's multi_logloss: 0.667773\n","[700]\tvalid_0's multi_logloss: 0.439057\tvalid_1's multi_logloss: 0.658935\n","[800]\tvalid_0's multi_logloss: 0.413621\tvalid_1's multi_logloss: 0.654829\n","[900]\tvalid_0's multi_logloss: 0.378107\tvalid_1's multi_logloss: 0.651918\n","[1000]\tvalid_0's multi_logloss: 0.351965\tvalid_1's multi_logloss: 0.652321\n","1 fold complete ################################\n","\n","[100]\tvalid_0's multi_logloss: 0.729532\tvalid_1's multi_logloss: 0.780185\n","[200]\tvalid_0's multi_logloss: 0.648001\tvalid_1's multi_logloss: 0.726115\n","[300]\tvalid_0's multi_logloss: 0.590496\tvalid_1's multi_logloss: 0.694461\n","[400]\tvalid_0's multi_logloss: 0.544351\tvalid_1's multi_logloss: 0.673411\n","[500]\tvalid_0's multi_logloss: 0.512577\tvalid_1's multi_logloss: 0.663777\n","[600]\tvalid_0's multi_logloss: 0.48281\tvalid_1's multi_logloss: 0.65515\n","[700]\tvalid_0's multi_logloss: 0.44577\tvalid_1's multi_logloss: 0.646621\n","[800]\tvalid_0's multi_logloss: 0.420848\tvalid_1's multi_logloss: 0.64398\n","[900]\tvalid_0's multi_logloss: 0.38563\tvalid_1's multi_logloss: 0.641078\n","[1000]\tvalid_0's multi_logloss: 0.359937\tvalid_1's multi_logloss: 0.640073\n","2 fold complete ################################\n","\n","[100]\tvalid_0's multi_logloss: 0.727141\tvalid_1's multi_logloss: 0.79466\n","[200]\tvalid_0's multi_logloss: 0.644628\tvalid_1's multi_logloss: 0.74353\n","[300]\tvalid_0's multi_logloss: 0.586057\tvalid_1's multi_logloss: 0.715921\n","[400]\tvalid_0's multi_logloss: 0.538595\tvalid_1's multi_logloss: 0.695579\n","[500]\tvalid_0's multi_logloss: 0.506456\tvalid_1's multi_logloss: 0.686885\n","[600]\tvalid_0's multi_logloss: 0.476331\tvalid_1's multi_logloss: 0.681176\n","[700]\tvalid_0's multi_logloss: 0.43846\tvalid_1's multi_logloss: 0.672951\n","[800]\tvalid_0's multi_logloss: 0.413368\tvalid_1's multi_logloss: 0.670078\n","[900]\tvalid_0's multi_logloss: 0.378338\tvalid_1's multi_logloss: 0.667854\n","[1000]\tvalid_0's multi_logloss: 0.352662\tvalid_1's multi_logloss: 0.667985\n","3 fold complete ################################\n","\n","[100]\tvalid_0's multi_logloss: 0.71988\tvalid_1's multi_logloss: 0.815058\n","[200]\tvalid_0's multi_logloss: 0.637212\tvalid_1's multi_logloss: 0.770251\n","[300]\tvalid_0's multi_logloss: 0.578601\tvalid_1's multi_logloss: 0.746026\n","[400]\tvalid_0's multi_logloss: 0.531626\tvalid_1's multi_logloss: 0.729811\n","[500]\tvalid_0's multi_logloss: 0.499056\tvalid_1's multi_logloss: 0.723775\n","[600]\tvalid_0's multi_logloss: 0.469467\tvalid_1's multi_logloss: 0.719303\n","[700]\tvalid_0's multi_logloss: 0.432082\tvalid_1's multi_logloss: 0.715957\n","[800]\tvalid_0's multi_logloss: 0.407626\tvalid_1's multi_logloss: 0.714832\n","[900]\tvalid_0's multi_logloss: 0.37257\tvalid_1's multi_logloss: 0.716798\n","[1000]\tvalid_0's multi_logloss: 0.346343\tvalid_1's multi_logloss: 0.719622\n","4 fold complete ################################\n","\n","0.6742893830660625"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltJCy4J33LV5"},"source":["# df_train = pd.read_csv('open/train.csv')\n","# df_test = pd.read_csv('open/test.csv')\n","# sub = pd.read_csv('open/sample_submission.csv')\n","\n","# # df_train = df_train.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","# # df_test = df_test.sort_values('begin_month', ascending=True).reset_index(drop=True)\n","\n","# df_train.loc[df_train['child_num'] > df_train['family_size'], 'family_size'] = 4\n","# df_train.loc[(df_train['family_type']=='Married') & (df_train['family_size']<2), 'family_size'] = 3\n","# df_train.loc[df_train['child_num'] == df_train['family_size'], 'family_size'] = 2\n","\n","# df_test.loc[df_test['child_num'] > df_test['family_size'], 'family_size'] = 4\n","# df_test.loc[(df_test['family_type']=='Married') & (df_test['family_size']<2), 'family_size'] = 3\n","\n","# # df_train = df_train.loc[df_train.loc[:, 'gender':'begin_month'].drop_duplicates().index].reset_index(drop=True)\n","# # df_train = df_train.loc[:, 'gender':'credit'].drop_duplicates().reset_index(drop=True)\n","\n","# temp = []\n","# duplicates = []\n","# for idx in tqdm(range(len(df_train))):\n","#     row = '_'.join(df_train.loc[idx, 'gender':'family_size'].astype(str).tolist())\n","#     # row = '_'.join(df_train.loc[idx, ['DAYS_BIRTH', 'DAYS_EMPLOYED']].astype(str).tolist())\n","#     # row = '_'.join(df_train.loc[idx, 'car':'family_size'].astype(str).tolist())\n","#     if row in temp:\n","#         # duplicates.append(np.isin(temp, row).sum() + 1)\n","#         duplicates.append(sum([row==i for i in temp]) + 1)\n","#     else:\n","#         duplicates.append(1)\n","#     temp.append(row)\n","\n","# df_train['rank'] = duplicates\n","# df_train['uid_rows'] = temp\n","\n","# duplicates = []\n","# for idx in tqdm(range(len(df_test))):\n","#     row = '_'.join(df_test.loc[idx, 'gender':'family_size'].astype(str).tolist())\n","#     if row in temp:\n","#         # duplicates.append(np.isin(temp, row).sum() + 1)\n","#         duplicates.append(sum([row==i for i in temp]) + 1)\n","#     else:\n","#         duplicates.append(1)\n","#     temp.append(row)\n","\n","# df_test['rank'] = duplicates\n","# df_test['uid_rows'] = temp[-10000:]\n","\n","train = df_train.copy()\n","test = df_test.copy()\n","\n","# 부양가족\n","# train['n_dependents'] = train['family_size'] - train['family_type'].apply(lambda x: 2 if 'Marr' in x else 1)\n","# test['n_dependents'] = test['family_size'] - test['family_type'].apply(lambda x: 2 if 'Marr' in x else 1)\n","train['n_dependents2'] = train['family_size'] - train['child_num']\n","test['n_dependents2'] = test['family_size'] - test['child_num']\n","# train['n_dependents3'] = train['n_dependents'] - train['child_num']\n","# test['n_dependents3'] = test['n_dependents'] - test['child_num']\n","\n","TARGET = 'credit'\n","\n","# categorical \n","for df in [train, test]:\n","    df['occyp_type'].fillna('NULL', inplace=True)\n","    df['reality'] = df['reality'].astype('category')\n","    df['family_type'] = df['family_type'].astype('category')\n","    df['gender'] = df['gender'].map({'F':0, 'M':1})\n","\n","for col in ['car', 'edu_type', 'house_type', 'occyp_type', 'income_type', 'work_phone', 'phone', 'email']:\n","    temp = train[col].value_counts(True).to_dict()\n","    for df in [train, test]:\n","        df[col] = df[col].map(temp)\n","\n","test['uid_rows_vc'] = test['uid_rows'].map(train['uid_rows'].value_counts())\n","train['uid_rows_vc'] = train['uid_rows'].map(train['uid_rows'].value_counts())\n","\n","train['DAYS_EMPLOYED_binary'] = train['DAYS_EMPLOYED']>0\n","test['DAYS_EMPLOYED_binary'] = test['DAYS_EMPLOYED']>0\n","\n","train['DAYS_BIRTH_DATE'] = train['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","test['DAYS_BIRTH_DATE'] = test['DAYS_BIRTH'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=25153, freq='d')[::-1])})\n","\n","# 미고용 상태 NULL\n","train['DAYS_EMPLOYED_DATE'] = train['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","test['DAYS_EMPLOYED_DATE'] = test['DAYS_EMPLOYED'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=15714, freq='d')[::-1])})\n","train['DAYS_EMPLOYED_DATE'] = pd.to_datetime(train['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","test['DAYS_EMPLOYED_DATE'] = pd.to_datetime(test['DAYS_EMPLOYED_DATE'].fillna('2020-01-01'))\n","\n","\n","train['begin_month_DATE'] = train['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","test['begin_month_DATE'] = test['begin_month'].abs().map({k:v for k,v in enumerate(pd.date_range(end='2019-12-31', periods=61, freq='m')[::-1])})\n","\n","for df in [train, test]:\n","\n","    # 2019년 기준 현재 나이, 정년까지 남은 연수\n","    df['age'] = 2019 - df['DAYS_BIRTH_DATE'].dt.year\n","    df['age2'] = 60 - df['age']\n","    # 2019년 기준 일한 햇수\n","    # df['EMPLOYED_year'] = (2019 - df['DAYS_EMPLOYED_DATE'].dt.year)\n","    # df['age3'] = df['age'] - df['age3']\n","    # df['age3'] = df['age'] - (2019 - df['begin_month_DATE'].dt.year)\n","    # df['loan_age'] = df['age'] - (2019 - df['begin_month_year'])\n","    # df['loan_age'] = df['loan_age']//10\n","\n","    df['DAYS_BIRTH_week'] = df['DAYS_BIRTH_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_BIRTH_day'] = df['DAYS_BIRTH_DATE'].dt.day\n","    df['DAYS_BIRTH_weekday'] = df['DAYS_BIRTH_DATE'].dt.weekday\n","\n","    df['DAYS_EMPLOYED_10y'] = df['DAYS_EMPLOYED_DATE'].dt.year//10\n","    df['DAYS_EMPLOYED_DATE_month'] = df['DAYS_EMPLOYED_DATE'].dt.month\n","    df['DAYS_EMPLOYED_DATE_week'] = df['DAYS_EMPLOYED_DATE'].dt.isocalendar().week.astype(int)\n","    df['DAYS_EMPLOYED_DATE_day'] = df['DAYS_EMPLOYED_DATE'].dt.day\n","    df['DAYS_EMPLOYED_DATE_weekday'] = df['DAYS_EMPLOYED_DATE'].dt.weekday\n","\n","    df['employment_days'] = (df['DAYS_BIRTH_DATE'] - df['DAYS_EMPLOYED_DATE']).dt.days\n","    df['employment_days'] = df['employment_days']//12\n","\n","    df.loc[df['DAYS_EMPLOYED_DATE']=='2020-01-01', ['DAYS_EMPLOYED_DATE_month', 'DAYS_EMPLOYED_DATE_week', 'DAYS_EMPLOYED_DATE_day', 'DAYS_EMPLOYED_DATE_weekday', 'employment_days']] = -1\n","\n","    df['begin_month_year'] = df['begin_month_DATE'].dt.year\n","    df['begin_month_month'] = df['begin_month_DATE'].dt.month\n","    \n","    # df['diff_DAYS_EMPLOYED_begin_month_month'] = (df['begin_month_year']*12 + df['begin_month_month']) - (df['DAYS_EMPLOYED_DATE'].dt.month*12 + df['DAYS_EMPLOYED_DATE_month'])\n","    # df['diff_DAYS_EMPLOYED_begin_month_month'] = (df['diff_DAYS_EMPLOYED_begin_month_month']/12).round()\n","\n","# for col in ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'income_total']:\n","#     for df in [train, test]:\n","#         df[col] = np.log1p(df[col])\n","\n","def create_features(train, test, uid, feature, aggs):\n","    tr, te = train.copy(), test.copy()\n","    \n","    if len(uid)==3:\n","        uid1, uid2, uid3 = uid[0], uid[1], uid[2]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str) + '_' + tr[uid3].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str) + '_' + te[uid3].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{uid3}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    \n","    elif len(uid)==2:\n","        uid1, uid2 = uid[0], uid[1]\n","        tr['uid'] = tr[uid1].astype(str) + '_' + tr[uid2].astype(str)\n","        te['uid'] = te[uid1].astype(str) + '_' + te[uid2].astype(str)\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}_{uid2}_{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","    else:\n","        uid1 = uid[0]\n","        tr['uid'] = tr[uid1].astype(str) + '_'\n","        te['uid'] = te[uid1].astype(str) + '_'\n","\n","        for agg in aggs:\n","            new_col = f'{uid1}__{feature}_{agg}'\n","            tr[new_col] = tr['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","            te[new_col] = te['uid'].map(tr.groupby('uid')[feature].agg(agg))\n","\n","    tr = tr.drop(columns='uid')\n","    te = te.drop(columns='uid')\n","\n","    return tr, te\n","\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month', ['mean', 'std', 'max'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_year', ['mean', 'std'])\n","train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED'], 'begin_month_month', ['mean', 'std'])\n","train, test = create_features(train, test, ['uid_rows'], 'begin_month_year', ['mean'])\n","\n","# train, test = create_features(train, test, ['DAYS_BIRTH'], 'begin_month_month', ['mean'])\n","# train, test = create_features(train, test, ['DAYS_EMPLOYED'], 'DAYS_BIRTH', ['mean'])\n","\n","# train, test = create_features(train, test, ['DAYS_BIRTH', 'DAYS_EMPLOYED_binary'], 'begin_month_month', ['mean'])\n","# train, test = create_features(train, test, ['uid_rows'], 'begin_month_month', ['mean',])\n","# train, test = create_features(train, test, 'DAYS_BIRTH', 'begin_month', 'DAYS_EMPLOYED_binary', ['mean'])\n","# train, test = create_features(train, test, 'DAYS_BIRTH', 'DAYS_EMPLOYED_binary', 'begin_month_month', ['std'])\n","# train, test = create_features(train, test, 'DAYS_BIRTH', 'DAYS_EMPLOYED_DATE_month', 'begin_month_month', ['mean'])\n","# train, test = create_features(train, test, 'DAYS_BIRTH', 'occyp_type', 'begin_month', ['mean'])\n","\n","missing_features = [col for col in train.columns if train[col].isnull().sum()>0]\n","print('missing_features :', missing_features)\n","for col in missing_features:\n","    train[col] = train[col].fillna(-99)\n","    test[col] = test[col].fillna(-99)\n","\n","##### 현재 시점 2019년이라고 가정\n","drop_col = ['index', 'FLAG_MOBIL', 'DAYS_BIRTH_DATE', 'DAYS_EMPLOYED_DATE', 'begin_month_DATE', 'DAYS_EMPLOYED_binary', 'DAYS_EMPLOYED', 'DAYS_BIRTH',\n","            'begin_month_year', 'begin_month_month', 'uid_rows', 'rank']\n","            \n"],"execution_count":null,"outputs":[]}]}