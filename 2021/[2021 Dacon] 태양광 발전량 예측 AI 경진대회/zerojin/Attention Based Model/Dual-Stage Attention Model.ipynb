{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QR2g2_iYDTy",
    "outputId": "6990bae7-40e7-4870-9af4-f5755088e5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# google drive와 colab연동\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WFxgRUjYGAE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from math import cos,radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_38Icz3YRfN"
   },
   "outputs": [],
   "source": [
    "def random_seed_fix(seed_num=42):\n",
    "  np.random.seed(seed_num)\n",
    "  random.seed(seed_num)\n",
    "  tf.compat.v1.set_random_seed(seed_num)\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),config=session_conf)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "random_seed_fix(seed_num=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIru0ZA8YSc6",
    "outputId": "4a33c376-0cbb-453b-930e-d2fa82daffcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:47<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"./gdrive/My Drive/dacon/발전량/data/\"\n",
    "\n",
    "train_df = pd.read_csv(path+'./train/train.csv')\n",
    "sub_df = pd.read_csv(path+'./sample_submission.csv')\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(os.listdir(path+'./test/')):\n",
    "    df = pd.read_csv(path+\"./test/\"+i)\n",
    "    df[\"F_NAME\"] = \"TEST_\"+i\n",
    "    test_df = pd.concat([test_df,df],axis=0)\n",
    "    del df\n",
    "\n",
    "test_df[\"TY\"] = test_df[\"F_NAME\"].apply(lambda x:x[x.find(\"_\")+1:x.find(\".\")])\n",
    "test_df[\"TY\"] = test_df[\"TY\"].astype('int')\n",
    "test_df.sort_values(by=[\"TY\",\"Day\",\"Hour\",\"Minute\"],inplace=True)\n",
    "test_df = test_df.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwdFwKluYTU4"
   },
   "outputs": [],
   "source": [
    "def sun_check(df):\n",
    "    que = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if (df[\"DHI\"][i]>0) & (df[\"DHI\"][i]>0):\n",
    "            que = 1\n",
    "\n",
    "        elif (df[\"DHI\"][i] ==0) & (df[\"DHI\"][i]==0):\n",
    "            que = 0\n",
    "\n",
    "        df.loc[i,\"sun_check\"] = que \n",
    "        \n",
    "    df[\"sun_check\"] = df[\"sun_check\"].astype('int')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def Altitude_check(cc):\n",
    "    \n",
    "    up_unit = round((69-45) / (cc.loc[:cc[cc['Altitude']==69].index[0],'Altitude'].isnull().sum()),2)\n",
    "    down_unit = round((69-45) / (cc.loc[cc[cc['Altitude']==69].index[0]:,'Altitude'].isnull().sum()),2)\n",
    "\n",
    "    que = 0\n",
    "\n",
    "    for i in range(1,len(cc)-1):\n",
    "        if (cc['Altitude'][i-1] == 0) & np.isnan(cc['Altitude'][i]) & (que == 0):\n",
    "            cc.loc[i,'Altitude'] = 45\n",
    "            que = 1\n",
    "\n",
    "        elif (cc['Altitude'][i-1] >= 45) & np.isnan(cc['Altitude'][i]) & (que == 1):\n",
    "            cc.loc[i,'Altitude'] = cc['Altitude'][i-1] + up_unit\n",
    "\n",
    "        elif (cc['Altitude'][i] == 69) & (cc['Altitude'][i] >= 0):\n",
    "            que = 2 \n",
    "            continue\n",
    "            \n",
    "        elif (cc['Altitude'][i-1] <= 69) & np.isnan(cc['Altitude'][i]) & (que == 2) & (cc['Altitude'][i+1] == 0) :\n",
    "            cc.loc[i,'Altitude'] = 45\n",
    "        \n",
    "        elif (cc['Altitude'][i-1] <= 69) & np.isnan(cc['Altitude'][i]) & (que == 2):\n",
    "            cc.loc[i,'Altitude'] = cc['Altitude'][i-1] - down_unit\n",
    "            \n",
    "    return cc\n",
    "    \n",
    "\n",
    "def Altitude(df,train=True):\n",
    "    \n",
    "    if train == True:\n",
    "        df = sun_check(df)\n",
    "        \n",
    "        all_df = pd.DataFrame()\n",
    "\n",
    "        df.loc[(df[\"Hour\"]==12) & (df[\"Minute\"]==30),\"Altitude\"] = 69\n",
    "        df.loc[(df[\"sun_check\"]==0),\"Altitude\"] = 0\n",
    "\n",
    "        for i in df[\"Day\"].unique():\n",
    "            cc_fe = Altitude_check(df[df['Day']==i].reset_index(drop=True))\n",
    "            all_df = pd.concat([all_df,cc_fe],axis=0).reset_index(drop=True)\n",
    "            \n",
    "        all_df['Altitude_diff'] = 90 - all_df['Altitude']\n",
    "        all_df.loc[all_df['Altitude_diff']==90,\"Altitude_diff\"]=0\n",
    "        all_df['GHI'] = all_df['DHI'] + (all_df['DNI'] *[cos(radians(i)) for i in all_df['Altitude_diff'].values])\n",
    "\n",
    "        return all_df\n",
    "\n",
    "    else:\n",
    "        df = sun_check(df)\n",
    "        all_df = pd.DataFrame()\n",
    "\n",
    "        df.loc[(df[\"Hour\"]==12) & (df[\"Minute\"]==30),\"Altitude\"] = 69\n",
    "        df.loc[(df[\"sun_check\"]==0),\"Altitude\"] = 0\n",
    "        \n",
    "        for i in test_df_fe[\"TY\"].unique():\n",
    "            temp = test_df_fe[test_df_fe[\"TY\"]==i].reset_index(drop=True)\n",
    "\n",
    "            for j in temp[\"Day\"].unique():\n",
    "                cc_fe = Altitude_check(temp[temp['Day'] == j].reset_index(drop=True))\n",
    "                all_df = pd.concat([all_df,cc_fe],axis=0).reset_index(drop=True)\n",
    "        \n",
    "        all_df['Altitude_diff'] = 90 - all_df['Altitude']\n",
    "        all_df.loc[all_df['Altitude_diff']==90,\"Altitude_diff\"]=0\n",
    "        all_df['GHI'] = all_df['DHI'] + (all_df['DNI'] *[cos(radians(i)) for i in all_df['Altitude_diff'].values])\n",
    "        \n",
    "        return all_df\n",
    "\n",
    "def time_all(df):\n",
    "    set_df = df.copy()\n",
    "\n",
    "    # 변수 만들기 끝\n",
    "    set_df[\"ALL_TIME\"] = set_df[\"Hour\"]*60 + set_df[\"Minute\"]\n",
    "    set_df[\"ALL_TIME\"] = set_df[\"ALL_TIME\"].astype('category')\n",
    "\n",
    "    return set_df\n",
    "\n",
    "def time_m(df):\n",
    "\n",
    "    m1 = [1,2,3,4,5,6]\n",
    "    m2 = [7,8,9,10,11,12]\n",
    "    m3 = [13,14,15,16,17,18]\n",
    "    m4 = [19,20,21,22,23,0]\n",
    "\n",
    "    df.loc[df[\"Hour\"].isin(m1),\"M_GROUP\"] = 1\n",
    "    df.loc[df[\"Hour\"].isin(m2),\"M_GROUP\"] = 2\n",
    "    df.loc[df[\"Hour\"].isin(m3),\"M_GROUP\"] = 3\n",
    "    df.loc[df[\"Hour\"].isin(m4),\"M_GROUP\"] = 4\n",
    "\n",
    "    df[\"M_GROUP\"] = df[\"M_GROUP\"].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def dark(df):\n",
    "    df.loc[df['TARGET']==0,'DARK'] = 1\n",
    "    df.loc[df['TARGET']!=0,'DARK'] = 0\n",
    "    \n",
    "    df['DARK'] = df['DARK'].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def time_function(train_df,test_df):\n",
    "\n",
    "    train_df_fe = time_all(train_df)\n",
    "    test_df_fe = time_all(test_df)\n",
    "\n",
    "    train_df_fe = time_m(train_df_fe)\n",
    "    test_df_fe = time_m(test_df_fe)\n",
    "    \n",
    "    train_df_fe = dark(train_df_fe)\n",
    "    test_df_fe = dark(test_df_fe)\n",
    "    \n",
    "    train_df_fe['DARK'] = train_df_fe['DARK'].astype('category') \n",
    "    test_df_fe['DARK'] = test_df_fe['DARK'].astype('category') \n",
    "    \n",
    "    return train_df_fe, test_df_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLrP_wswYUe-"
   },
   "outputs": [],
   "source": [
    "# 새로운 데이터 셋\n",
    "train_df_fe,test_df_fe = time_function(train_df,test_df)\n",
    "\n",
    "train_df_fe = Altitude(train_df_fe,train=True)\n",
    "test_df_fe = Altitude(test_df_fe,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwxbn-9fYVk8",
    "outputId": "03f3ffcd-8915-41b4-da9b-fa7875a1a399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 변수 : 16개 \n",
      " ['Day' 'Hour' 'Minute' 'DHI' 'DNI' 'WS' 'RH' 'T' 'TARGET' 'ALL_TIME'\n",
      " 'M_GROUP' 'DARK' 'sun_check' 'Altitude' 'Altitude_diff' 'GHI'] \n",
      "\n",
      "제거할 변수 : 7개 \n",
      " ['Day', 'DARK', 'Hour', 'Minute', 'sun_check', 'ALL_TIME', 'M_GROUP'] \n",
      "\n",
      "제거 된 뒤 현재 변수 : 9개 \n",
      " ['DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'Altitude', 'Altitude_diff', 'GHI'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#del_col = ['Day','DARK', 'Hour', 'Minute', 'sun_check',]\n",
    "del_col = ['Day','DARK', 'Hour', 'Minute', 'sun_check','ALL_TIME','M_GROUP']\n",
    "\n",
    "print(\"현재 변수 : {}개 \\n {} \\n\".format(len(train_df_fe.columns),train_df_fe.columns.values))\n",
    "print(\"제거할 변수 : {}개 \\n {} \\n\".format(len(del_col),del_col))\n",
    "\n",
    "train_df_fe.drop(del_col,axis='columns',inplace=True)\n",
    "\n",
    "use_col = list(train_df_fe.columns.values)\n",
    "\n",
    "print(\"제거 된 뒤 현재 변수 : {}개 \\n {} \\n\".format(len(use_col),use_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B5cXXFefshS"
   },
   "outputs": [],
   "source": [
    "def day_range2(df,train_range,predict_range):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    d_timestamp = 48\n",
    "    all_day = len(df)//d_timestamp\n",
    "\n",
    "    for i in range(0,all_day-train_range-predict_range+1,1):\n",
    "        _x = df.iloc[(i*48):(i+train_range)*48,:].values\n",
    "        _y = df.iloc[(i+train_range)*48:(i+train_range+predict_range)*48,5].values\n",
    "\n",
    "        x.append(np.ravel(_x.T))\n",
    "        y.append(_y)\n",
    "        \n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D8j8g7Bfub6"
   },
   "outputs": [],
   "source": [
    "train_range = 5\n",
    "predict_range = 2\n",
    "\n",
    "x, y  = day_range2(train_df_fe, train_range, predict_range)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXshLAdCe0rz",
    "outputId": "9a064a74-fc43-4796-8c1c-ca8122835944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 2160)\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    \n",
    "    data = []\n",
    "    tmp = test_df_fe[test_df_fe['TY'] == i].reset_index(drop=True)\n",
    "    tmp = tmp.loc[:, use_col].values\n",
    "    tmp = tmp[-((train_range+predict_range)*48):-(predict_range*48),:]\n",
    "    data.append(np.ravel(tmp.T))\n",
    "    data = np.array(data)\n",
    "    test.append(data)\n",
    "\n",
    "test = np.concatenate(test, axis=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SknT5tPifjZS",
    "outputId": "dbea5537-1ba6-45e5-99ee-b199b7c93a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 1, 1580)\n",
      "(1089, 96)\n",
      "(81, 1, 1580)\n"
     ]
    }
   ],
   "source": [
    "# n_unique 1 제거\n",
    "not_unique_features = pd.DataFrame(x).nunique()>1\n",
    "\n",
    "x = x[:, not_unique_features]\n",
    "test = test[:, not_unique_features]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_s = MinMaxScaler()\n",
    "y_s = MinMaxScaler()\n",
    "\n",
    "x = x_s.fit_transform(x)\n",
    "y = y_s.fit_transform(y)\n",
    "test = x_s.fit_transform(test)\n",
    "\n",
    "x = x.reshape(1089,1,-1)\n",
    "test = test.reshape(81,1,-1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAm2dn2nhbTJ"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7jQGgRGYvQx"
   },
   "outputs": [],
   "source": [
    "class Encoder_lstm(Layer):\n",
    "    def __init__(self, m):\n",
    "\n",
    "        super(Encoder_lstm, self).__init__(name=\"encoder_lstm\")\n",
    "        self.lstm = LSTM(m, return_state=True)\n",
    "        self.initial_state = None\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "\n",
    "        h_s, _, c_s = self.lstm(x, initial_state = self.initial_state)\n",
    "        self.initial_state = [h_s, c_s]\n",
    "\n",
    "        return h_s, c_s\n",
    "\n",
    "    def reset_state(self, h0, c0):\n",
    "        self.initial_state = [h0, c0]\n",
    "\n",
    "\n",
    "class Input_Attention(Layer):\n",
    "    def __init__(self,T):\n",
    "        super(Input_Attention,self).__init__(name = \"input_attention\")\n",
    "        self.w1 = Dense(T)\n",
    "        self.w2 = Dense(T)\n",
    "        self.v = Dense(1)\n",
    "\n",
    "    def call(self, h_s, c_s, x):\n",
    "\n",
    "        query = tf.concat([h_s,c_s],axis=-1)\n",
    "        query = RepeatVector(x.shape[2])(query)\n",
    "\n",
    "        x_perm = Permute((2,1))(x)\n",
    "\n",
    "        score = tf.nn.tanh(self.w1(x_perm) + self.w2(query))\n",
    "        score = self.v(score)\n",
    "        score = Permute((2,1))(score)\n",
    "        attention_weigths = tf.nn.softmax(score)\n",
    "\n",
    "        return attention_weigths\n",
    "\n",
    "\n",
    "class Encoder(Layer):\n",
    "    def __init__(self,T,m):\n",
    "        super(Encoder,self).__init__(name = 'encoder')\n",
    "        self.T = T\n",
    "        self.input_att = Input_Attention(T)\n",
    "        self.lstm = Encoder_lstm(m)\n",
    "        self.alpha_t = None\n",
    "\n",
    "    def call(self, data, h0, c0, training=False):\n",
    "\n",
    "        self.lstm.reset_state(h0=h0, c0=c0)\n",
    "\n",
    "        alpha_seq = tf.TensorArray(tf.float32, self.T)\n",
    "\n",
    "        for t in range(self.T):\n",
    "\n",
    "            x = data[:, t:t+1, :]\n",
    "            h_s,c_s = self.lstm(x)\n",
    "\n",
    "            self.alpha_t = self.input_att(h_s,c_s,data)\n",
    "\n",
    "            alpha_seq = alpha_seq.write(t, self.alpha_t)\n",
    "\n",
    "        alpha_seq = tf.reshape(alpha_seq.stack(), (-1, self.T, data.shape[2]))\n",
    "        output = tf.multiply(data,alpha_seq)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAM2bMfthoQL"
   },
   "outputs": [],
   "source": [
    "class input_att_model(Model):\n",
    "    def __init__(self, T, m):\n",
    "        super(input_att_model, self).__init__(name=\"input_att_model\")\n",
    "\n",
    "        self.m = m\n",
    "        self.encoder = Encoder(T=T, m=m)\n",
    "        \n",
    "        self.lstm = LSTM(m)\n",
    "\n",
    "        self.dense1 = Dense(1024)\n",
    "        self.dense2 = Dense(512)\n",
    "        self.dense3 = Dense(256)\n",
    "        self.dense4 = Dense(96)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "\n",
    "\n",
    "        batch = inputs.shape[0]\n",
    "\n",
    "        h0 = tf.zeros((batch, self.m))\n",
    "        c0 = tf.zeros((batch, self.m))\n",
    "\n",
    "        enc_output = self.encoder(\n",
    "                                inputs, \n",
    "                                h0=h0, c0=c0, \n",
    "                                training=training,\n",
    "                                )  \n",
    "        \n",
    "        enc_h = self.lstm(enc_output)  \n",
    "        out1 = self.dense1(enc_h)\n",
    "        out2 = self.dense2(out1)\n",
    "        out3 = self.dense3(out2)\n",
    "        out_put = self.dense4(out3)\n",
    "        \n",
    "        return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SguxNTpwqlQC"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OM_snqji79S"
   },
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyIqTi-1v4u8"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "\terr = (y_true - y_pred)\n",
    "\treturn K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTI0kUrfxR-R"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "kfold = KFold(n_splits=5,random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "score_dict = {}\n",
    "ensemble_dict = {}\n",
    "\n",
    "for q in tqdm(quantiles):\n",
    "    \n",
    "    print(\"Quantile : \", q)\n",
    "    random_seed_fix(seed_num=42)\n",
    "\n",
    "\n",
    "    for j in tqdm(range(1,8)):\n",
    "        print(\"DAY_RANGE :\",j)\n",
    "        \n",
    "        ############## 데이터 전처리 ###############\n",
    "\n",
    "        train_range = j\n",
    "        predict_range = 2\n",
    "\n",
    "        x, y  = day_range2(train_df_fe, train_range, predict_range)\n",
    "\n",
    "        for i in range(81):\n",
    "            data = []\n",
    "            tmp = test_df_fe[test_df_fe['TY'] == i].reset_index(drop=True)\n",
    "            tmp = tmp.loc[:, use_col].values\n",
    "            tmp = tmp[-(train_range*48):,:]\n",
    "            data.append(np.ravel(tmp.T))\n",
    "            data = np.array(data)\n",
    "         \n",
    "            \n",
    "        # n_unique 1 제거\n",
    "        not_unique_features = pd.DataFrame(x).nunique()>1\n",
    "\n",
    "        x = x[:, not_unique_features]\n",
    "\n",
    "        x_s = MinMaxScaler()\n",
    "        y_s = MinMaxScaler()\n",
    "\n",
    "        x = x_s.fit_transform(x)\n",
    "        y = y_s.fit_transform(y)\n",
    "\n",
    "        x = x.reshape(x.shape[0],1,-1)\n",
    "\n",
    "        \n",
    "        ############## 모델링 ###############\n",
    "\n",
    "        k = 1\n",
    "        for train_index, test_index in tqdm(kfold.split(x)):\n",
    "\n",
    "            train_x = x[train_index]\n",
    "            train_y = y[train_index]\n",
    "            valid_x = x[test_index]\n",
    "            valid_y = y[test_index]\n",
    "\n",
    "            model = input_att_model(T=1,m=512,)\n",
    "            model.build((128,1, train_x.shape[2]))\n",
    "\n",
    "            tf.config.run_functions_eagerly(True)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss = lambda y_true, y_pred: quantile_loss(q, y_true, y_pred), optimizer=opt, metrics=[lambda y, pred: quantile_loss(q, y, pred)])\n",
    "            \n",
    "            filename = 'checkpoint-q-{}-dayrange-{}-trial_{}_-001.h5'.format(q, j, k)\n",
    "\n",
    "            # 콜백 정의\n",
    "            reduceLR = ReduceLROnPlateau(\n",
    "                monitor='val_loss',  \n",
    "                factor=0.9,          \n",
    "                patience=30,         \n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            checkpoint = ModelCheckpoint(filename,             \n",
    "                                        monitor='val_loss',   \n",
    "                                        verbose=0,            \n",
    "                                        save_best_only = True, \n",
    "                                        mode='min'           \n",
    "                                        )\n",
    "\n",
    "            earlystopping = EarlyStopping(monitor='val_loss',  \n",
    "                                        patience=10,         \n",
    "                                        verbose = 1,\n",
    "                                        )\n",
    "\n",
    "            model.fit(train_x, train_y, \n",
    "                    batch_size = 128, \n",
    "                    epochs = 100, \n",
    "                    verbose = 0, validation_data = (valid_x, valid_y),\n",
    "                    callbacks = [checkpoint, earlystopping, reduceLR]\n",
    "              )\n",
    "            \n",
    "            print(filename)\n",
    "            model.load_weights(filename)            \n",
    "            preds_test =  y_s.inverse_transform(model.predict(test))\n",
    "            k += 1\n",
    "            test_data_list.append(preds_test)\n",
    "\n",
    "    pd.DataFrame(sum(test_data_list)/len(test_data_list)).to_csv(path+\"앙상블_{}.csv\".format(q),index=False)\n",
    "    \n",
    "    score_dict[q] = np.mean(cv_loss_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lr7p4KPpn1P"
   },
   "outputs": [],
   "source": [
    "all_df.to_csv(path+\"last.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_nn_발전량 최종버전.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
