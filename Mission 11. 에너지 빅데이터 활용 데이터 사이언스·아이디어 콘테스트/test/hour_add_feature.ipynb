{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hour_add_feature.ipynb","provenance":[],"collapsed_sections":["xA_6PpluI56F"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"C0DtaFDhdlXB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"outputId":"37fee105-c6f3-42b5-d0a0-5e974371b045","executionInfo":{"status":"ok","timestamp":1571819605107,"user_tz":-540,"elapsed":5412,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}}},"source":["# install\n","!pip install workalendar\n","from workalendar.asia import SouthKorea\n","\n","# !pip3 install tsfresh\n","# from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters ,ComprehensiveFCParameters"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: workalendar in /usr/local/lib/python3.6/dist-packages (7.0.0)\n","Requirement already satisfied: pyluach in /usr/local/lib/python3.6/dist-packages (from workalendar) (1.0.1)\n","Requirement already satisfied: skyfield in /usr/local/lib/python3.6/dist-packages (from workalendar) (1.13)\n","Requirement already satisfied: pyCalverter in /usr/local/lib/python3.6/dist-packages (from workalendar) (1.6.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from workalendar) (2.5.3)\n","Requirement already satisfied: lunardate in /usr/local/lib/python3.6/dist-packages (from workalendar) (0.2.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from workalendar) (2018.9)\n","Requirement already satisfied: skyfield-data in /usr/local/lib/python3.6/dist-packages (from workalendar) (0.1.0)\n","Requirement already satisfied: setuptools>=1.0 in /usr/local/lib/python3.6/dist-packages (from workalendar) (41.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from skyfield->workalendar) (1.16.5)\n","Requirement already satisfied: sgp4>=1.4 in /usr/local/lib/python3.6/dist-packages (from skyfield->workalendar) (1.4)\n","Requirement already satisfied: jplephem>=2.3 in /usr/local/lib/python3.6/dist-packages (from skyfield->workalendar) (2.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->workalendar) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nFpHQC0260Oc","colab_type":"code","outputId":"2e05d56c-19bd-4c30-b9e9-242cfa6d4a9e","executionInfo":{"status":"ok","timestamp":1571819605881,"user_tz":-540,"elapsed":5994,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import gc\n","import tqdm\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.neural_network import MLPRegressor\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import Lasso, Ridge\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold\n","\n","# model\n","import xgboost as xgb\n","import lightgbm as lgb\n","\n","# evaluation\n","from sklearn.metrics import mean_squared_error"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pJc8YZ3k63Rc","colab_type":"code","outputId":"f091ebf7-9561-42a7-8a8e-eb1cc1f100f9","executionInfo":{"status":"ok","timestamp":1571819605884,"user_tz":-540,"elapsed":2112,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = 'drive/My Drive/11dacon/data/'\n","\n","def seed_everything(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    \n","from numba import jit\n","import math\n","\n","@jit\n","def smape_fast(y_true, y_pred, exp=True):\n","    \n","    if exp:\n","        y_true = np.expm1(np.array(y_true))\n","        y_pred = np.expm1(np.array(y_pred))\n","    else:\n","        y_true = np.array(y_true)\n","        y_pred = np.array(y_pred)\n","        \n","    out = 0\n","    for i in range(y_true.shape[0]):\n","        a = y_true[i]\n","        b = y_pred[i]\n","        c = a+b\n","        if c == 0:\n","            continue\n","        out += math.fabs(a - b) / c\n","    out *= (200.0 / y_true.shape[0])\n","    return out\n","\n","def rmse(y_true, y_pred, exp=True):\n","    if exp:\n","        return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n","    else:\n","        return np.sqrt(mean_squared_error(y_true, y_pred))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0WnZkSUA7Ar1","colab_type":"code","colab":{}},"source":["train = pd.read_csv(path+'train.csv')\n","test = pd.read_csv(path+'test.csv')\n","sub = pd.read_csv(path+'submission.csv')\n","\n","holidays = pd.concat([pd.Series(np.array(SouthKorea().holidays(2018))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2017))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2016))[:, 0])]).reset_index(drop=True)\n","\n","weather3 = pd.read_csv(path+'weather_hour.csv', encoding='cp949').iloc[:, [1, 2, 3, 4, 5, 7, 8]]\n","weather3.columns = ['일시', '기온', '강수량', '풍속', '습도', '날씨', '전운량']\n","\n","weather3['날씨'] = LabelEncoder().fit_transform(weather3['날씨'].fillna('no value'))\n","weather3 = weather3.fillna(0)\n","weather3['일시'] = pd.to_datetime(weather3['일시'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-NWlZYWD80C","colab_type":"code","colab":{}},"source":["def merge(train, test_out=False):\n","    temp2 = pd.DataFrame()\n","    train_df = pd.DataFrame()\n","    test_df = pd.DataFrame()\n","    for col in train.columns[1:]:\n","        temp = train[['Time', col]].dropna().rename(columns={col:'target'})\n","        temp['house'] = int(col.replace('X', ''))\n","        temp = temp[temp['Time']>='2018-01-01'].reset_index(drop=True)\n","\n","        temp['Time'] = pd.to_datetime(temp['Time'])\n","        temp['month'] = temp['Time'].dt.month\n","        temp['date'] = temp['Time'].dt.date\n","        temp['holiday'] = temp['date'].isin(holidays).astype(int)\n","        temp['week'] = temp['Time'].dt.week\n","        temp['weekday'] = temp['Time'].dt.weekday\n","        temp['hour'] = temp['Time'].dt.hour\n","        temp['working_hour'] = temp['hour'].map({0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, \n","                                                        9:1, 10:1, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1, 17:1, \n","                                                        18:1, 19:1, 20:1, 21:0, 22:0, 23:0, 24:0})\n","        temp['is_weekend'] = temp['Time'].dt.weekday.map({0:0, 1:0, 2:0, 3:0, 4:0, 5:1, 6:1})\n","        temp['is_weekend_holiday'] = (temp['is_weekend']+temp['holiday']).map({0:0, 1:1, 2:1})\n","        \n","        if test_out:\n","            temp2 = pd.DataFrame(pd.date_range('2018-07-01', '2018-07-02', freq='h'), columns=['Time']).loc[:23]\n","            temp2['house'] = int(col.replace('X', ''))\n","            temp2 = temp2[temp2['Time']>='2018-01-01'].reset_index(drop=True)\n","\n","            temp2['Time'] = pd.to_datetime(temp2['Time'])\n","            temp2['month'] = temp2['Time'].dt.month\n","            temp2['date'] = temp2['Time'].dt.date\n","            temp2['holiday'] = temp2['date'].isin(holidays).astype(int)\n","            temp2['week'] = temp2['Time'].dt.week\n","            temp2['weekday'] = temp2['Time'].dt.weekday\n","            temp2['hour'] = temp2['Time'].dt.hour\n","            temp2['working_hour'] = temp2['hour'].map({0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, \n","                                                            9:1, 10:1, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1, 17:1, \n","                                                            18:1, 19:1, 20:1, 21:0, 22:0, 23:0, 24:0})\n","            temp2['is_weekend'] = temp['Time'].dt.weekday.map({0:0, 1:0, 2:0, 3:0, 4:0, 5:1, 6:1})\n","            temp2['is_weekend_holiday'] = (temp2['is_weekend']+temp2['holiday']).map({0:0, 1:1, 2:1})\n","            \n","            temp2['target_mean'] = temp['target'].mean()\n","            temp2['target_min'] = temp['target'].min()\n","            temp2['target_max'] = temp['target'].max()\n","            temp2['target_std'] = temp['target'].std()\n","            temp2['target_quan1'] = temp['target'].quantile(.25)\n","            temp2['target_quan2'] = temp['target'].quantile(.5)\n","            temp2['target_quan3'] = temp['target'].quantile(.75)\n","            temp2['target_quan4'] = temp['target'].quantile(.125)\n","            temp2['target_quan5'] = temp['target'].quantile(.375)\n","            temp2['target_quan6'] = temp['target'].quantile(.675)\n","            temp2['target_quan7'] = temp['target'].quantile(.925)\n","        else:\n","            temp['target_mean'] = temp['target'].mean()\n","            temp['target_min'] = temp['target'].min()\n","            temp['target_max'] = temp['target'].max()\n","            temp['target_std'] = temp['target'].std()\n","            temp['target_quan1'] = temp['target'].quantile(.25)\n","            temp['target_quan2'] = temp['target'].quantile(.5)\n","            temp['target_quan3'] = temp['target'].quantile(.75)\n","            temp['target_quan4'] = temp['target'].quantile(.125)\n","            temp['target_quan5'] = temp['target'].quantile(.375)\n","            temp['target_quan6'] = temp['target'].quantile(.675)\n","            temp['target_quan7'] = temp['target'].quantile(.925)\n","\n","        \n","        def local_merge(temp2, condition=False, test_out=False, name='temp'):\n","            if test_out:\n","                temp2[name] = temp2['week'].map(condition)\n","                return temp2\n","            else:\n","                return 0\n","            \n","        temp_week = temp.groupby('week')['target'].mean().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_mean'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        \n","        temp_week = temp.groupby('week')['target'].min().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_min'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('week')['target'].max().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name='prior_target_max'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('week')['target'].std().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_std'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('week')['target'].quantile(.25).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_quan1'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('week')['target'].quantile(.5).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_quan2'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('week')['target'].quantile(.75).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['week'] = temp_week['week']+1\n","        temp_week = temp_week.set_index('week').to_dict()['prior_target']\n","        name = 'prior_target_quan3'\n","        temp[name] = temp['week'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        def local_merge(temp2=False, condition=False, test_out=False, name=False):\n","            if test_out:\n","                temp2[name] = temp2['month'].map(condition)\n","                return temp2\n","            else:\n","                return 0\n","            \n","        temp_week = temp.groupby('month')['target'].mean().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_mean_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].min().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_min_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].max().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_max_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].std().reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_std_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].quantile(.25).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name= 'prior_target_quan1_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].quantile(.5).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_quan2_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp_week = temp.groupby('month')['target'].quantile(.75).reset_index().rename(columns={'target':'prior_target'})\n","        temp_week['month'] = temp_week['month']+1\n","        temp_week = temp_week.set_index('month').to_dict()['prior_target']\n","        name = 'prior_target_quan3_month'\n","        temp[name] = temp['month'].map(temp_week)\n","        temp2 = local_merge(temp2, temp_week, test_out, name)\n","        \n","        temp = temp.dropna()\n","        \n","        if test_out:\n","            test_df = pd.concat([test_df, temp2]).reset_index(drop=True)\n","        else:\n","            train_df = pd.concat([train_df, temp]).reset_index(drop=True)\n","    \n","    if test_out:\n","        test_df.rename(columns={'Time':'일시'}, inplace=True)\n","        test_df = pd.merge(test_df, weather3, how='left', on='일시')\n","        return test_df\n","    else:\n","        train_df.rename(columns={'Time':'일시'}, inplace=True)\n","        train_df = pd.merge(train_df, weather3, how='left', on='일시')\n","        return train_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2f8vw2_pc-qf","colab_type":"code","colab":{}},"source":["# train_df2 = merge(train)\n","train_df = merge(test)\n","\n","# train_df = pd.concat([train_df, train_df2]).reset_index(drop=True)\n","\n","train_df['target'] = np.log1p(train_df['target'])\n","\n","test_df = train_df[train_df['일시']>='2018-06-30 00'].reset_index(drop=True)\n","train_df = train_df[train_df['일시']<'2018-06-30 00'].reset_index(drop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J30nO_LxoSK9","colab_type":"code","colab":{}},"source":["# train_df['Time'] = train_df['Time'].astype(str)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEyeDhTLqI3e","colab_type":"code","colab":{}},"source":["# from tsfresh import extract_features\n","# extracted_features = extract_features(train_df.reset_index(drop=True).reset_index()[['index', 'target', 'Time']], column_id=\"index\", column_sort=\"Time\", n_jobs=16, chunksize=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6w90hapVoSq9","colab_type":"code","colab":{}},"source":["# from tsfresh import select_features\n","# from tsfresh.utilities.dataframe_functions import impute\n","\n","# impute(extracted_features)\n","# features_filtered = select_features(extracted_features, train_df.reset_index(drop=True).reset_index()['target'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xA_6PpluI56F","colab_type":"text"},"source":["##### XGB"]},{"cell_type":"code","metadata":{"id":"lrli4zqKTgR3","colab_type":"code","colab":{}},"source":["params = {\n","    'objective':'reg:squarederror',\n","    'n_estimators':10000,\n","    'max_depth':2**3,\n","    'learning_rate':0.03,\n","    'n_jobs':-1,\n","    'seed':42\n","}\n","\n","oof = np.zeros(len(train_df))\n","pred = np.zeros(len(test_df))\n","\n","feature = [i for i in train_df.columns if i not in ['target', '일시', 'date']]\n","kf = KFold(n_splits=5, random_state=42, shuffle=False)\n","gkf = GroupKFold(n_splits=5)\n","best_iterations = []\n","\n","for trn_idx, val_idx in kf.split(train_df):\n","# for trn_idx, val_idx in gkf.split(train_df, groups=train_df['house']):\n","    tt = xgb.DMatrix(train_df.loc[trn_idx, feature], train_df.loc[trn_idx, ['target']])\n","    vv = xgb.DMatrix(train_df.loc[val_idx, feature], train_df.loc[val_idx, ['target']])\n","    \n","    model = xgb.train(params, tt, num_boost_round=5000, evals=[(tt, 'train'), (vv, 'val')], early_stopping_rounds=200, verbose_eval=0)\n","    \n","    oof[val_idx] = model.predict(xgb.DMatrix(train_df.loc[val_idx, feature]))\n","    pred += model.predict(xgb.DMatrix(test_df[feature]))/5\n","    best_iterations.append(model.best_iteration)\n","smape_fast(test_df['target'], pred, False), rmse(test_df['target'], pred, False), best_iterations, np.mean(best_iterations)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OytkGLmqXtr_","colab_type":"code","colab":{}},"source":["# predictions\n","train_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n","params = {\n","    'objective':'reg:squarederror',\n","    'n_estimators':10000,\n","    'max_depth':2**3,\n","    'learning_rate':0.03,\n","    'n_jobs':-1,\n","    'seed':42\n","}\n","\n","temp = pd.DataFrame(pd.Series(pd.date_range('2018-07-01', '2018-11-01')).dt.to_period('m').unique(), columns=['일시'])\n","temp = pd.merge(temp, weather1, how='left', on='일시')\n","\n","test_df = pd.DataFrame()\n","for col in sub['meter_id'].apply(lambda x: x.split('X')[1]):\n","    temp['house'] = int(col)\n","    test_df = pd.concat([test_df, temp])\n","test_df = pd.merge(test_df, day_range, how='left', on='일시')\n","test_df['num_day'] = test_df['평일'] + test_df['주말_공휴일']\n","test_df = pd.merge(test_df, train_df[['house', 'target_mean', 'target_min', 'target_max', 'target_std', 'target_quan1', 'target_quan2', 'target_quan3']].drop_duplicates(), how='left', on='house')\n","\n","test_df = test_df.reset_index(drop=True)\n","pred = np.zeros(len(test_df))\n","\n","for seed in [42, 43, 44, 45, 46]:\n","    params['seed']=seed\n","    \n","    tt = xgb.DMatrix(train_df[feature], train_df[['target']])\n","    model = xgb.train(params, tt, num_boost_round=250, evals=[(tt, 'train')], verbose_eval=100)\n","    pred += model.predict(xgb.DMatrix(test_df[feature]))/5\n","pred_xgb = pred.copy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jrWD0u9SI9B2","colab_type":"text"},"source":["##### LGB"]},{"cell_type":"code","metadata":{"id":"n-RbM4hnLmyk","colab_type":"code","outputId":"17fdcf45-c54e-4d31-d767-7890c00b39d4","executionInfo":{"status":"ok","timestamp":1571813806742,"user_tz":-540,"elapsed":3385530,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["params = {\n","    'objective':'regression',\n","    'boosting_type':'gbdt',\n","    'metric':'rmse',\n","    'n_jobs':-1,\n","    'learning_rate':0.03,\n","    'num_leaves': 2**8,\n","    'max_depth':-1,\n","    'tree_learner':'serial',\n","    'colsample_bytree': 0.7,\n","    'subsample_freq':1,\n","    'subsample':0.7,\n","    'reg_alpha':0.1,\n","    'reg_lambda':0.1,\n","    'n_estimators':10000,\n","    'max_bin':255,\n","    'verbose':-1,\n","    'seed': 42,\n","    'early_stopping_rounds':100\n","}\n","\n","oof = np.zeros(len(train_df))\n","pred = np.zeros(len(test_df))\n","\n","feature = [i for i in train_df.columns if i not in ['target', '일시', 'date', 'month', 'week']]\n","kf = KFold(n_splits=5, random_state=42, shuffle=False)\n","gkf = GroupKFold(n_splits=5)\n","best_iterations = []\n","\n","for trn_idx, val_idx in kf.split(train_df):\n","    tt = lgb.Dataset(train_df.loc[trn_idx, feature], train_df.loc[trn_idx, ['target']])\n","    vv = lgb.Dataset(train_df.loc[val_idx, feature], train_df.loc[val_idx, ['target']])\n","    \n","    model = lgb.train(params, tt, valid_sets=[tt, vv], early_stopping_rounds=200, verbose_eval=0)\n","    \n","    pred += model.predict(test_df[feature])/5\n","    best_iterations.append(model.best_iteration)\n","    print('HH')\n","    del model\n","    gc.collect()\n","    \n","smape_fast(test_df['target'], pred, False), rmse(test_df['target'], pred, False), best_iterations, np.mean(best_iterations)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:123: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"],"name":"stderr"},{"output_type":"stream","text":["HH\n","HH\n","HH\n","HH\n","HH\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(37.08619982149266, 0.19718739171855024, [240, 879, 1169, 1399, 1462], 1029.8)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"ul-CtB8JGIw0","colab_type":"code","colab":{}},"source":["# week : overfit\n","# all summary statistics : 41.9\n","# prior weekly summary statistics : 40.32\n","\n","# using train, test dataset : 37.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6o5jvTFLnhv","colab_type":"code","outputId":"3fa375fa-023a-4d3b-e58f-40e92a4ce66c","executionInfo":{"status":"ok","timestamp":1571818978216,"user_tz":-540,"elapsed":1901884,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["# predictions\n","train_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n","test_df = merge(test, test_out=True)\n","\n","pred = np.zeros(len(test_df))\n","\n","params['n_estimators']=1050\n","for seed in [42, 43, 44, 45, 46]:\n","    params['seed']=seed\n","    \n","    tt = lgb.Dataset(train_df[feature], train_df[['target']])\n","    model = lgb.train(params, tt, valid_sets=[tt], verbose_eval=300)\n","    pred += model.predict(test_df[feature])/5\n","    del model\n","    gc.collect()\n","pred_lgb = pred.copy()"],"execution_count":41,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:123: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"],"name":"stderr"},{"output_type":"stream","text":["Training until validation scores don't improve for 100 rounds.\n","[300]\ttraining's rmse: 0.122338\n","[600]\ttraining's rmse: 0.117379\n","[900]\ttraining's rmse: 0.114166\n","Did not meet early stopping. Best iteration is:\n","[1050]\ttraining's rmse: 0.112878\n","Training until validation scores don't improve for 100 rounds.\n","[300]\ttraining's rmse: 0.122389\n","[600]\ttraining's rmse: 0.117264\n","[900]\ttraining's rmse: 0.114041\n","Did not meet early stopping. Best iteration is:\n","[1050]\ttraining's rmse: 0.112793\n","Training until validation scores don't improve for 100 rounds.\n","[300]\ttraining's rmse: 0.122225\n","[600]\ttraining's rmse: 0.117219\n","[900]\ttraining's rmse: 0.114048\n","Did not meet early stopping. Best iteration is:\n","[1050]\ttraining's rmse: 0.112842\n","Training until validation scores don't improve for 100 rounds.\n","[300]\ttraining's rmse: 0.122347\n","[600]\ttraining's rmse: 0.117404\n","[900]\ttraining's rmse: 0.114182\n","Did not meet early stopping. Best iteration is:\n","[1050]\ttraining's rmse: 0.112907\n","Training until validation scores don't improve for 100 rounds.\n","[300]\ttraining's rmse: 0.122294\n","[600]\ttraining's rmse: 0.117415\n","[900]\ttraining's rmse: 0.114154\n","Did not meet early stopping. Best iteration is:\n","[1050]\ttraining's rmse: 0.112902\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2zfJpsSGUlbd","colab_type":"text"},"source":["##### submission"]},{"cell_type":"code","metadata":{"id":"uOf3Y_r4XbA5","colab_type":"code","colab":{}},"source":["# pred = pred_lgb*0.5 + pred_xgb*0.5\n","test_df['pred'] = np.expm1(pred_lgb)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVFYtBR-XbI1","colab_type":"code","colab":{}},"source":["sub_df = test_df.groupby(['house', '일시'])['pred'].sum().unstack().reset_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CDIKEI6RXbNK","colab_type":"code","outputId":"f8b89e47-58cf-48e0-af43-6e92793db030","executionInfo":{"status":"error","timestamp":1571819103179,"user_tz":-540,"elapsed":1387,"user":{"displayName":"이현호","photoUrl":"","userId":"01233170983161563057"}},"colab":{"base_uri":"https://localhost:8080/","height":376}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = 'drive/My Drive/11dacon/submit/'\n","\n","sub_df.to_csv(path+'hour_predicion_lgb.csv', index=False)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-a2a246a7de92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/My Drive/11dacon/submit/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msub_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'hour_predicion_lgb.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3018\u001b[0m         \u001b[0;31m# this could be a view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m         \u001b[0;31m# but only in a single-dtyped view slicable case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3020\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mUnicodeWriter\u001b[0;34m(f, dialect, encoding, **kwds)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \"\"\"\n\u001b[1;32m    544\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mCSV\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0mrows\u001b[0m \u001b[0mto\u001b[0m \u001b[0mCSV\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mwhich\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \"\"\"\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: lineterminator must be set"]}]},{"cell_type":"code","metadata":{"id":"YZOtzlUl8Vqe","colab_type":"code","colab":{}},"source":["sub_df.to_pickle(path+'aaa.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tw4XrBTX7_qe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfsuaWXU7_tW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}